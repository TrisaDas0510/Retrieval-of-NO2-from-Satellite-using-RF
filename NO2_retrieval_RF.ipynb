{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrisaDas0510/Retrieval-of-NO2-from-Satellite-using-RF/blob/main/NO2_retrieval_RF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz6OOsvAh7GP",
        "outputId": "caff0551-6e56-4296-926b-e2de73c9ad6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 148.3497\n",
            "R-squared Score: 0.5891\n",
            "Plots have been saved as 'feature_importance.png' and 'actual_vs_predicted.png'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Converting Date to datetime\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Handling missing values by filling with mean for numerical columns\n",
        "    numerical_columns = ['NO2_Ground', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                        'Temperature', 'Wind_Speed', 'NO2_Satellite']\n",
        "    for col in numerical_columns:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "    # Dropping rows where Wind_Direction is missing\n",
        "    df = df.dropna(subset=['Wind_Direction'])\n",
        "\n",
        "    # Selecting features and target\n",
        "    features = ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                'Temperature', 'Wind_Speed', 'Wind_Direction']\n",
        "    X = df[features]\n",
        "    y = df['NO2_Ground']\n",
        "\n",
        "    return X, y, df\n",
        "\n",
        "# Training the Random Forest model\n",
        "def train_random_forest(X, y):\n",
        "    # Splitting the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initializing and training the model\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Making predictions\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "\n",
        "    # Calculating performance metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return rf_model, X_train, X_test, y_train, y_test, y_pred, mse, r2\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, features):\n",
        "    importance = model.feature_importances_\n",
        "    feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Feature Importance in Random Forest Model')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual NO2 Ground Concentration')\n",
        "    plt.ylabel('Predicted NO2 Ground Concentration')\n",
        "    plt.title('Actual vs Predicted NO2 Ground Concentrations')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('actual_vs_predicted.png')\n",
        "    plt.close()\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "    X, y, df = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Train the model\n",
        "    rf_model, X_train, X_test, y_train, y_test, y_pred, mse, r2 = train_random_forest(X, y)\n",
        "\n",
        "    # Print performance metrics\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"R-squared Score: {r2:.4f}\")\n",
        "\n",
        "    # Plot feature importance\n",
        "    plot_feature_importance(rf_model, X.columns)\n",
        "\n",
        "    # Plot actual vs predicted values\n",
        "    plot_actual_vs_predicted(y_test, y_pred)\n",
        "\n",
        "    print(\"Plots have been saved as 'feature_importance.png' and 'actual_vs_predicted.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SjERPMpjXbA",
        "outputId": "4a4a5164-6e78-4da9-e834-4d6b117e6721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 5.0049\n",
            "R-squared Score: 0.5551\n",
            "Cross-validated R-squared Scores: [-0.10459572  0.1261106   0.10374922 -0.09729397  0.07781373]\n",
            "Mean CV R-squared: 0.0212\n",
            "Plots have been saved as 'feature_importance_improved.png' and 'actual_vs_predicted_improved.png'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Converting Date to datetime\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Handling duplicates by averaging\n",
        "    df = df.groupby(['Date', 'Station']).mean().reset_index()\n",
        "\n",
        "    # Handling missing values\n",
        "    numerical_columns = ['NO2_Ground', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                        'Temperature', 'Wind_Speed', 'NO2_Satellite']\n",
        "    for col in numerical_columns:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "    df['Wind_Direction'] = df['Wind_Direction'].fillna(df['Wind_Direction'].median())\n",
        "\n",
        "    # Removing outliers using IQR method\n",
        "    def remove_outliers(df, column):\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "    df = remove_outliers(df, 'NO2_Ground')\n",
        "\n",
        "    # Feature engineering\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['Temp_RH_Interaction'] = df['Temperature'] * df['Relative_Humidity']\n",
        "    df['Wind_Speed_Direction'] = df['Wind_Speed'] * np.cos(np.radians(df['Wind_Direction']))\n",
        "\n",
        "    # Selecting features and target\n",
        "    features = ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month',\n",
        "                'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
        "    X = df[features]\n",
        "    y = df['NO2_Ground']\n",
        "\n",
        "    # Scaling features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y, df, features, scaler\n",
        "\n",
        "# Training the Random Forest model with hyperparameter tuning\n",
        "def train_random_forest(X, y):\n",
        "    # Splitting the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Defining parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2]\n",
        "    }\n",
        "\n",
        "    # Initializing model\n",
        "    rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "    # Performing grid search\n",
        "    grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Making predictions\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculating performance metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Cross-validation scores\n",
        "    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "    return best_model, X_train, X_test, y_train, y_test, y_pred, mse, r2, cv_scores, grid_search\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, features):\n",
        "    importance = model.feature_importances_\n",
        "    feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Feature Importance in Random Forest Model')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance_improved.png')\n",
        "    plt.close()\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual NO2 Ground Concentration')\n",
        "    plt.ylabel('Predicted NO2 Ground Concentration')\n",
        "    plt.title('Actual vs Predicted NO2 Ground Concentrations')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('actual_vs_predicted_improved.png')\n",
        "    plt.close()\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "    X, y, df, features, scaler = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Train the model\n",
        "    best_model, X_train, X_test, y_train, y_test, y_pred, mse, r2, cv_scores, grid_search = train_random_forest(X, y)\n",
        "\n",
        "    # Print performance metrics\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"R-squared Score: {r2:.4f}\")\n",
        "    print(f\"Cross-validated R-squared Scores: {cv_scores}\")\n",
        "    print(f\"Mean CV R-squared: {cv_scores.mean():.4f}\")\n",
        "\n",
        "    # Plot feature importance\n",
        "    plot_feature_importance(best_model, features)\n",
        "\n",
        "    # Plot actual vs predicted values\n",
        "    plot_actual_vs_predicted(y_test, y_pred)\n",
        "\n",
        "    print(\"Plots have been saved as 'feature_importance_improved.png' and 'actual_vs_predicted_improved.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the data\n",
        "def load_and_preprocess_data(file_path, station):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Filter by station\n",
        "        df = df[df['Station'] == station]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available for station {station}.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Converting Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Select numeric columns for averaging, excluding non-numeric columns\n",
        "        numeric_columns = ['NO2_Ground', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                          'Temperature', 'Wind_Speed', 'Wind_Direction', 'NO2_Satellite']\n",
        "        df_numeric = df[['Date'] + [col for col in numeric_columns if col in df.columns]]\n",
        "\n",
        "        # Handling duplicates by averaging numeric columns\n",
        "        df_numeric = df_numeric.groupby('Date').mean().reset_index()\n",
        "\n",
        "        # Re-merge with non-numeric columns if needed (e.g., Station for reference)\n",
        "        df = df.drop_duplicates(subset=['Date'])[['Date']].merge(df_numeric, on='Date', how='left')\n",
        "\n",
        "        # Handling missing values\n",
        "        for col in numeric_columns:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].mean())\n",
        "        if 'Wind_Direction' in df.columns:\n",
        "            df['Wind_Direction'] = df['Wind_Direction'].fillna(df['Wind_Direction'].median())\n",
        "\n",
        "        # Less aggressive outlier removal (using 2 * IQR)\n",
        "        def remove_outliers(df, column):\n",
        "            if column in df.columns and not df[column].empty:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 2 * IQR\n",
        "                upper_bound = Q3 + 2 * IQR\n",
        "                return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "            return df\n",
        "\n",
        "        df = remove_outliers(df, 'NO2_Ground')\n",
        "\n",
        "        # Feature engineering\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Temp_RH_Interaction'] = df['Temperature'] * df['Relative_Humidity']\n",
        "        if 'Wind_Speed' in df.columns and 'Wind_Direction' in df.columns:\n",
        "            df['Wind_Speed_Direction'] = df['Wind_Speed'] * np.cos(np.radians(df['Wind_Direction']))\n",
        "        else:\n",
        "            df['Wind_Speed_Direction'] = 0  # Default value if Wind_Direction is missing\n",
        "\n",
        "        # Selecting features and target\n",
        "        features = ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                    'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month',\n",
        "                    'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
        "        # Filter features present in df\n",
        "        features = [f for f in features if f in df.columns]\n",
        "        if not features or 'NO2_Ground' not in df.columns:\n",
        "            print(f\"Insufficient features or target for station {station}.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        X = df[features]\n",
        "        y = df['NO2_Ground']\n",
        "\n",
        "        # Scaling features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        return X_scaled, y, df, features, scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing data for station {station}: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Training the Random Forest model with hyperparameter tuning\n",
        "def train_random_forest(X, y):\n",
        "    try:\n",
        "        # Splitting the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Defining expanded parameter grid\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['sqrt', 0.33]  # Removed 'auto' to avoid warning\n",
        "        }\n",
        "\n",
        "        # Initializing model\n",
        "        rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "        # Performing grid search with KFold\n",
        "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        grid_search = GridSearchCV(rf_model, param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Best model\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        # Making predictions\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        # Calculating performance metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Cross-validation scores\n",
        "        cv_scores = cross_val_score(best_model, X, y, cv=kfold, scoring='r2')\n",
        "\n",
        "        return best_model, X_train, X_test, y_train, y_test, y_pred, mse, r2, cv_scores, grid_search\n",
        "    except Exception as e:\n",
        "        print(f\"Error training model: {str(e)}\")\n",
        "        return None, None, None, None, None, None, None, None, None\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, features, station):\n",
        "    try:\n",
        "        importance = model.feature_importances_\n",
        "        feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "        plt.title(f'Feature Importance in Random Forest Model ({station})')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'feature_importance_{station}.png')\n",
        "        plt.close()\n",
        "\n",
        "        return feature_importance.head(3)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting feature importance for {station}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred, station):\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual NO2 Ground Concentration')\n",
        "        plt.ylabel('Predicted NO2 Ground Concentration')\n",
        "        plt.title(f'Actual vs Predicted NO2 Ground Concentrations ({station})')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'actual_vs_predicted_{station}.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting actual vs predicted for {station}: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "    stations = ['Agrabad', 'Tv_Center']\n",
        "\n",
        "    for station in stations:\n",
        "        print(f\"\\nProcessing data for station: {station}\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        X, y, df, features, scaler = load_and_preprocess_data(file_path, station)\n",
        "\n",
        "        if X is None or y is None or len(df) < 10:\n",
        "            print(f\"Skipping {station} due to insufficient or invalid data.\")\n",
        "            continue\n",
        "\n",
        "        # Train the model\n",
        "        result = train_random_forest(X, y)\n",
        "        if result[0] is None:\n",
        "            print(f\"Skipping {station} due to training error.\")\n",
        "            continue\n",
        "\n",
        "        best_model, X_train, X_test, y_train, y_test, y_pred, mse, r2, cv_scores, grid_search = result\n",
        "\n",
        "        # Print performance metrics\n",
        "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "        print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "        print(f\"R-squared Score: {r2:.4f}\")\n",
        "        print(f\"Cross-validated R-squared Scores: {cv_scores}\")\n",
        "        print(f\"Mean CV R-squared: {cv_scores.mean():.4f}\")\n",
        "\n",
        "        # Plot feature importance and get top features\n",
        "        top_features = plot_feature_importance(best_model, features, station)\n",
        "        if top_features is not None:\n",
        "            print(f\"Top 3 Features for {station}:\\n{top_features}\")\n",
        "\n",
        "        # Plot actual vs predicted values\n",
        "        plot_actual_vs_predicted(y_test, y_pred, station)\n",
        "\n",
        "        print(f\"Plots saved as 'feature_importance_{station}.png' and 'actual_vs_predicted_{station}.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHji0cd9w38i",
        "outputId": "07ed4fc3-3da7-4f67-f3da-f886e8912f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing data for station: Agrabad\n",
            "Best Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 15.2211\n",
            "R-squared Score: 0.6185\n",
            "Cross-validated R-squared Scores: [0.62437744 0.60058049 0.64780061 0.59933524 0.71725341]\n",
            "Mean CV R-squared: 0.6379\n",
            "Top 3 Features for Agrabad:\n",
            "             Feature  Importance\n",
            "6              Month    0.307840\n",
            "4         Wind_Speed    0.135628\n",
            "1  Relative_Humidity    0.127209\n",
            "Plots saved as 'feature_importance_Agrabad.png' and 'actual_vs_predicted_Agrabad.png'\n",
            "\n",
            "Processing data for station: Tv_Center\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean Squared Error: 0.0000\n",
            "R-squared Score: 0.0000\n",
            "Cross-validated R-squared Scores: [  0.         -39.11111111 -39.11111111 -39.11111111 -39.11111111]\n",
            "Mean CV R-squared: -31.2889\n",
            "Top 3 Features for Tv_Center:\n",
            "             Feature  Importance\n",
            "0      NO2_Satellite         0.0\n",
            "1  Relative_Humidity         0.0\n",
            "2    Solar_Radiation         0.0\n",
            "Plots saved as 'feature_importance_Tv_Center.png' and 'actual_vs_predicted_Tv_Center.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the data\n",
        "def load_and_preprocess_data(file_path, station):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Filter by station\n",
        "        df = df[df['Station'] == station]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available for station {station}.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Check for constant target\n",
        "        if df['NO2_Ground'].nunique() <= 1:\n",
        "            print(f\"Target NO2_Ground is constant for station {station}. Skipping modeling.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Converting Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Select numeric columns for averaging\n",
        "        numeric_columns = ['NO2_Ground', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                          'Temperature', 'Wind_Speed', 'Wind_Direction', 'NO2_Satellite']\n",
        "        df_numeric = df[['Date'] + [col for col in numeric_columns if col in df.columns]]\n",
        "\n",
        "        # Handling duplicates by averaging numeric columns\n",
        "        df_numeric = df_numeric.groupby('Date').mean().reset_index()\n",
        "\n",
        "        # Merge back with Date\n",
        "        df = df.drop_duplicates(subset=['Date'])[['Date']].merge(df_numeric, on='Date', how='left')\n",
        "\n",
        "        # Handling missing values\n",
        "        for col in numeric_columns:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].mean())\n",
        "        if 'Wind_Direction' in df.columns:\n",
        "            df['Wind_Direction'] = df['Wind_Direction'].fillna(df['Wind_Direction'].median())\n",
        "\n",
        "        # Less aggressive outlier removal (2 * IQR)\n",
        "        def remove_outliers(df, column):\n",
        "            if column in df.columns and not df[column].empty:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 2 * IQR\n",
        "                upper_bound = Q3 + 2 * IQR\n",
        "                return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "            return df\n",
        "\n",
        "        df = remove_outliers(df, 'NO2_Ground')\n",
        "\n",
        "        # Feature engineering\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Temp_RH_Interaction'] = df['Temperature'] * df['Relative_Humidity']\n",
        "        if 'Wind_Speed' in df.columns and 'Wind_Direction' in df.columns:\n",
        "            df['Wind_Speed_Direction'] = df['Wind_Speed'] * np.cos(np.radians(df['Wind_Direction']))\n",
        "        else:\n",
        "            df['Wind_Speed_Direction'] = 0\n",
        "\n",
        "        # Selecting features and target\n",
        "        features = ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                    'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month',\n",
        "                    'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
        "        features = [f for f in features if f in df.columns]\n",
        "        if not features or 'NO2_Ground' not in df.columns:\n",
        "            print(f\"Insufficient features or target for station {station}.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        X = df[features]\n",
        "        y = df['NO2_Ground']\n",
        "\n",
        "        # Scaling features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        return X_scaled, y, df, features, scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing data for station {station}: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Training and comparing Random Forest and XGBoost models\n",
        "def train_models(X, y, station):\n",
        "    try:\n",
        "        # Splitting the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Random Forest\n",
        "        rf_param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [5, 10, 15],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'min_samples_leaf': [1, 2],\n",
        "            'max_features': ['sqrt', 0.5]\n",
        "        }\n",
        "        rf_model = RandomForestRegressor(random_state=42)\n",
        "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "        rf_grid_search.fit(X_train, y_train)\n",
        "        rf_best_model = rf_grid_search.best_estimator_\n",
        "\n",
        "        # XGBoost\n",
        "        xgb_param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        }\n",
        "        xgb_model = XGBRegressor(random_state=42)\n",
        "        xgb_grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "        xgb_grid_search.fit(X_train, y_train)\n",
        "        xgb_best_model = xgb_grid_search.best_estimator_\n",
        "\n",
        "        # Predictions and metrics\n",
        "        models = {'Random Forest': rf_best_model, 'XGBoost': xgb_best_model}\n",
        "        results = {}\n",
        "        for name, model in models.items():\n",
        "            y_pred = model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='r2')\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'y_pred': y_pred,\n",
        "                'mse': mse,\n",
        "                'r2': r2,\n",
        "                'cv_scores': cv_scores,\n",
        "                'best_params': rf_grid_search.best_params_ if name == 'Random Forest' else xgb_grid_search.best_params_\n",
        "            }\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, results\n",
        "    except Exception as e:\n",
        "        print(f\"Error training models for {station}: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Feature selection based on importance\n",
        "def select_important_features(model, features, X, threshold=0.05):\n",
        "    try:\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = model.feature_importances_\n",
        "            feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "            selected_features = feature_importance[feature_importance['Importance'] >= threshold]['Feature'].tolist()\n",
        "            if not selected_features:\n",
        "                return features, X\n",
        "            selected_indices = [features.index(f) for f in selected_features]\n",
        "            return selected_features, X[:, selected_indices]\n",
        "        return features, X\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature selection: {str(e)}\")\n",
        "        return features, X\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, features, station, model_name):\n",
        "    try:\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = model.feature_importances_\n",
        "            feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "            feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "            plt.title(f'Feature Importance in {model_name} Model ({station})')\n",
        "            plt.xlabel('Importance')\n",
        "            plt.ylabel('Feature')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'feature_importance_{station}_{model_name.lower().replace(\" \", \"_\")}.png')\n",
        "            plt.close()\n",
        "\n",
        "            return feature_importance.head(3)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting feature importance for {station} ({model_name}): {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred, station, model_name):\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual NO2 Ground Concentration')\n",
        "        plt.ylabel('Predicted NO2 Ground Concentration')\n",
        "        plt.title(f'Actual vs Predicted NO2 Ground Concentrations ({station}, {model_name})')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'actual_vs_predicted_{station}_{model_name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting actual vs predicted for {station} ({model_name}): {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "    stations = ['Agrabad']  # Excluding Tv_Center due to constant data\n",
        "\n",
        "    for station in stations:\n",
        "        print(f\"\\nProcessing data for station: {station}\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        X, y, df, features, scaler = load_and_preprocess_data(file_path, station)\n",
        "\n",
        "        if X is None or y is None or len(df) < 10:\n",
        "            print(f\"Skipping {station} due to insufficient or invalid data.\")\n",
        "            continue\n",
        "\n",
        "        # Train models\n",
        "        result = train_models(X, y, station)\n",
        "        if result[0] is None:\n",
        "            print(f\"Skipping {station} due to training error.\")\n",
        "            continue\n",
        "\n",
        "        X_train, X_test, y_train, y_test, results = result\n",
        "\n",
        "        # Process each model\n",
        "        for model_name, res in results.items():\n",
        "            print(f\"\\nResults for {model_name} ({station}):\")\n",
        "            print(f\"Best Parameters: {res['best_params']}\")\n",
        "            print(f\"Mean Squared Error: {res['mse']:.4f}\")\n",
        "            print(f\"R-squared Score: {res['r2']:.4f}\")\n",
        "            print(f\"Cross-validated R-squared Scores: {res['cv_scores']}\")\n",
        "            print(f\"Mean CV R-squared: {res['cv_scores'].mean():.4f}\")\n",
        "\n",
        "            # Feature selection\n",
        "            selected_features, X_selected = select_important_features(res['model'], features, X, threshold=0.05)\n",
        "            print(f\"Selected Features: {selected_features}\")\n",
        "\n",
        "            # Plot feature importance\n",
        "            top_features = plot_feature_importance(res['model'], selected_features, station, model_name)\n",
        "            if top_features is not None:\n",
        "                print(f\"Top 3 Features:\\n{top_features}\")\n",
        "\n",
        "            # Plot actual vs predicted\n",
        "            plot_actual_vs_predicted(y_test, res['y_pred'], station, model_name)\n",
        "\n",
        "            print(f\"Plots saved as 'feature_importance_{station}_{model_name.lower().replace(' ', '_')}.png' and 'actual_vs_predicted_{station}_{model_name.lower().replace(' ', '_')}.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "798cO5U60sA3",
        "outputId": "3f95b265-2d5a-4d58-f74a-96f45a9f10cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing data for station: Agrabad\n",
            "\n",
            "Results for Random Forest (Agrabad):\n",
            "Best Parameters: {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 15.3311\n",
            "R-squared Score: 0.6157\n",
            "Cross-validated R-squared Scores: [0.61350302 0.62270001 0.6544858  0.6375751  0.7370683 ]\n",
            "Mean CV R-squared: 0.6531\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Wind_Speed', 'Month', 'Temp_RH_Interaction']\n",
            "Error plotting feature importance for Agrabad (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Agrabad_random_forest.png' and 'actual_vs_predicted_Agrabad_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Agrabad):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Mean Squared Error: 16.5551\n",
            "R-squared Score: 0.5851\n",
            "Cross-validated R-squared Scores: [0.61844366 0.50005039 0.66234864 0.60173751 0.70945308]\n",
            "Mean CV R-squared: 0.6184\n",
            "Selected Features: ['Relative_Humidity', 'Wind_Speed', 'Month']\n",
            "Error plotting feature importance for Agrabad (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Agrabad_xgboost.png' and 'actual_vs_predicted_Agrabad_xgboost.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the data\n",
        "def load_and_preprocess_data(file_path, station, min_samples=10):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Filter by station\n",
        "        df = df[df['Station'] == station]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available for station {station}.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Check for constant or insufficient target data\n",
        "        if df['NO2_Ground'].nunique() <= 1:\n",
        "            print(f\"Target NO2_Ground is constant for station {station}. Skipping modeling.\")\n",
        "            return None, None, None, None, None\n",
        "        if len(df) < min_samples:\n",
        "            print(f\"Insufficient samples ({len(df)}) for station {station}. Minimum required: {min_samples}.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Converting Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Select numeric columns for averaging\n",
        "        numeric_columns = ['NO2_Ground', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                          'Temperature', 'Wind_Speed', 'Wind_Direction', 'NO2_Satellite']\n",
        "        df_numeric = df[['Date'] + [col for col in numeric_columns if col in df.columns]]\n",
        "\n",
        "        # Handling duplicates by averaging numeric columns\n",
        "        df_numeric = df_numeric.groupby('Date').mean().reset_index()\n",
        "\n",
        "        # Merge back with Date\n",
        "        df = df.drop_duplicates(subset=['Date'])[['Date']].merge(df_numeric, on='Date', how='left')\n",
        "\n",
        "        # Handling missing values\n",
        "        for col in numeric_columns:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].mean())\n",
        "        if 'Wind_Direction' in df.columns:\n",
        "            df['Wind_Direction'] = df['Wind_Direction'].fillna(df['Wind_Direction'].median())\n",
        "\n",
        "        # Check for constant features and remove them\n",
        "        for col in df.columns:\n",
        "            if col not in ['Date'] and df[col].nunique() <= 1:\n",
        "                print(f\"Feature {col} is constant for station {station}. Dropping.\")\n",
        "                df = df.drop(columns=[col])\n",
        "\n",
        "        # Less aggressive outlier removal (2 * IQR)\n",
        "        def remove_outliers(df, column):\n",
        "            if column in df.columns and not df[column].empty:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 2 * IQR\n",
        "                upper_bound = Q3 + 2 * IQR\n",
        "                return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "            return df\n",
        "\n",
        "        df = remove_outliers(df, 'NO2_Ground')\n",
        "\n",
        "        # Feature engineering\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        if 'Temperature' in df.columns and 'Relative_Humidity' in df.columns:\n",
        "            df['Temp_RH_Interaction'] = df['Temperature'] * df['Relative_Humidity']\n",
        "        else:\n",
        "            df['Temp_RH_Interaction'] = 0\n",
        "        if 'Wind_Speed' in df.columns and 'Wind_Direction' in df.columns:\n",
        "            df['Wind_Speed_Direction'] = df['Wind_Speed'] * np.cos(np.radians(df['Wind_Direction']))\n",
        "        else:\n",
        "            df['Wind_Speed_Direction'] = 0\n",
        "\n",
        "        # Selecting features and target\n",
        "        features = ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                    'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month',\n",
        "                    'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
        "        features = [f for f in features if f in df.columns]\n",
        "        if not features or 'NO2_Ground' not in df.columns:\n",
        "            print(f\"Insufficient features or target for station {station}.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        X = df[features]\n",
        "        y = df['NO2_Ground']\n",
        "\n",
        "        # Scaling features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        return X_scaled, y, df, features, scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing data for station {station}: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Training and comparing Random Forest and XGBoost models\n",
        "def train_models(X, y, station):\n",
        "    try:\n",
        "        # Splitting the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Random Forest\n",
        "        rf_param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [5, 10, 15],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'min_samples_leaf': [1, 2],\n",
        "            'max_features': ['sqrt', 0.5]\n",
        "        }\n",
        "        rf_model = RandomForestRegressor(random_state=42)\n",
        "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "        rf_grid_search.fit(X_train, y_train)\n",
        "        rf_best_model = rf_grid_search.best_estimator_\n",
        "\n",
        "        # XGBoost\n",
        "        xgb_param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        }\n",
        "        xgb_model = XGBRegressor(random_state=42)\n",
        "        xgb_grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "        xgb_grid_search.fit(X_train, y_train)\n",
        "        xgb_best_model = xgb_grid_search.best_estimator_\n",
        "\n",
        "        # Predictions and metrics\n",
        "        models = {'Random Forest': rf_best_model, 'XGBoost': xgb_best_model}\n",
        "        results = {}\n",
        "        for name, model in models.items():\n",
        "            y_pred = model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='r2')\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'y_pred': y_pred,\n",
        "                'mse': mse,\n",
        "                'r2': r2,\n",
        "                'cv_scores': cv_scores,\n",
        "                'best_params': rf_grid_search.best_params_ if name == 'Random Forest' else xgb_grid_search.best_params_\n",
        "            }\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, results\n",
        "    except Exception as e:\n",
        "        print(f\"Error training models for {station}: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Feature selection based on importance\n",
        "def select_important_features(model, features, X, threshold=0.05):\n",
        "    try:\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = model.feature_importances_\n",
        "            feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "            selected_features = feature_importance[feature_importance['Importance'] >= threshold]['Feature'].tolist()\n",
        "            if not selected_features:\n",
        "                return features, X\n",
        "            selected_indices = [features.index(f) for f in selected_features]\n",
        "            return selected_features, X[:, selected_indices]\n",
        "        return features, X\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature selection: {str(e)}\")\n",
        "        return features, X\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, features, station, model_name):\n",
        "    try:\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = model.feature_importances_\n",
        "            feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "            feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "            plt.title(f'Feature Importance in {model_name} Model ({station})')\n",
        "            plt.xlabel('Importance')\n",
        "            plt.ylabel('Feature')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'feature_importance_{station}_{model_name.lower().replace(\" \", \"_\")}.png')\n",
        "            plt.close()\n",
        "\n",
        "            return feature_importance.head(3)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting feature importance for {station} ({model_name}): {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred, station, model_name):\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual NO2 Ground Concentration')\n",
        "        plt.ylabel('Predicted NO2 Ground Concentration')\n",
        "        plt.title(f'Actual vs Predicted NO2 Ground Concentrations ({station}, {model_name})')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'actual_vs_predicted_{station}_{model_name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting actual vs predicted for {station} ({model_name}): {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "\n",
        "    # Load dataset to get all stations\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        stations = df['Station'].unique()\n",
        "        print(f\"Found stations: {list(stations)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    for station in stations:\n",
        "        print(f\"\\nProcessing data for station: {station}\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        X, y, df, features, scaler = load_and_preprocess_data(file_path, station)\n",
        "\n",
        "        if X is None or y is None or len(df) < 10:\n",
        "            print(f\"Skipping {station} due to insufficient or invalid data.\")\n",
        "            continue\n",
        "\n",
        "        # Train models\n",
        "        result = train_models(X, y, station)\n",
        "        if result[0] is None:\n",
        "            print(f\"Skipping {station} due to training error.\")\n",
        "            continue\n",
        "\n",
        "        X_train, X_test, y_train, y_test, results = result\n",
        "\n",
        "        # Process each model\n",
        "        for model_name, res in results.items():\n",
        "            print(f\"\\nResults for {model_name} ({station}):\")\n",
        "            print(f\"Best Parameters: {res['best_params']}\")\n",
        "            print(f\"Mean Squared Error: {res['mse']:.4f}\")\n",
        "            print(f\"R-squared Score: {res['r2']:.4f}\")\n",
        "            print(f\"Cross-validated R-squared Scores: {res['cv_scores']}\")\n",
        "            print(f\"Mean CV R-squared: {res['cv_scores'].mean():.4f}\")\n",
        "\n",
        "            # Feature selection\n",
        "            selected_features, X_selected = select_important_features(res['model'], features, X, threshold=0.05)\n",
        "            print(f\"Selected Features: {selected_features}\")\n",
        "\n",
        "            # Plot feature importance\n",
        "            top_features = plot_feature_importance(res['model'], selected_features, station, model_name)\n",
        "            if top_features is not None:\n",
        "                print(f\"Top 3 Features:\\n{top_features}\")\n",
        "\n",
        "            # Plot actual vs predicted\n",
        "            plot_actual_vs_predicted(y_test, res['y_pred'], station, model_name)\n",
        "\n",
        "            print(f\"Plots saved as 'feature_importance_{station}_{model_name.lower().replace(' ', '_')}.png' and 'actual_vs_predicted_{station}_{model_name.lower().replace(' ', '_')}.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk3NcaHY1Qij",
        "outputId": "88831593-9447-48b2-ebb1-b91bfd508d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found stations: ['Agrabad', 'BRAC', 'Barisal', 'Cumilla', 'Darussalam', 'DoE', 'Gazipur', 'Khulna', 'Mymensingh', 'NG', 'NS', 'Rajshahi', 'Rangpur', 'Savar', 'Sylhet', 'TV_Center', 'Tv_Center']\n",
            "\n",
            "Processing data for station: Agrabad\n",
            "\n",
            "Results for Random Forest (Agrabad):\n",
            "Best Parameters: {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 15.3311\n",
            "R-squared Score: 0.6157\n",
            "Cross-validated R-squared Scores: [0.61350302 0.62270001 0.6544858  0.6375751  0.7370683 ]\n",
            "Mean CV R-squared: 0.6531\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Wind_Speed', 'Month', 'Temp_RH_Interaction']\n",
            "Error plotting feature importance for Agrabad (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Agrabad_random_forest.png' and 'actual_vs_predicted_Agrabad_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Agrabad):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Mean Squared Error: 16.5551\n",
            "R-squared Score: 0.5851\n",
            "Cross-validated R-squared Scores: [0.61844366 0.50005039 0.66234864 0.60173751 0.70945308]\n",
            "Mean CV R-squared: 0.6184\n",
            "Selected Features: ['Relative_Humidity', 'Wind_Speed', 'Month']\n",
            "Error plotting feature importance for Agrabad (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Agrabad_xgboost.png' and 'actual_vs_predicted_Agrabad_xgboost.png'\n",
            "\n",
            "Processing data for station: BRAC\n",
            "\n",
            "Results for Random Forest (BRAC):\n",
            "Best Parameters: {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean Squared Error: 658.1003\n",
            "R-squared Score: 0.4984\n",
            "Cross-validated R-squared Scores: [0.45330686 0.62489872 0.55856372 0.73592428 0.65209119]\n",
            "Mean CV R-squared: 0.6050\n",
            "Selected Features: ['Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for BRAC (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_BRAC_random_forest.png' and 'actual_vs_predicted_BRAC_random_forest.png'\n",
            "\n",
            "Results for XGBoost (BRAC):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Mean Squared Error: 724.0564\n",
            "R-squared Score: 0.4481\n",
            "Cross-validated R-squared Scores: [0.4467472  0.61356255 0.50572281 0.73583751 0.62415878]\n",
            "Mean CV R-squared: 0.5852\n",
            "Selected Features: ['Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for BRAC (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_BRAC_xgboost.png' and 'actual_vs_predicted_BRAC_xgboost.png'\n",
            "\n",
            "Processing data for station: Barisal\n",
            "\n",
            "Results for Random Forest (Barisal):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 1.1628\n",
            "R-squared Score: 0.5991\n",
            "Cross-validated R-squared Scores: [0.59180431 0.64682626 0.57181289 0.64609788 0.72037918]\n",
            "Mean CV R-squared: 0.6354\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Barisal (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Barisal_random_forest.png' and 'actual_vs_predicted_Barisal_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Barisal):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Mean Squared Error: 1.0132\n",
            "R-squared Score: 0.6506\n",
            "Cross-validated R-squared Scores: [0.61851252 0.67620457 0.58224092 0.66595771 0.66884914]\n",
            "Mean CV R-squared: 0.6424\n",
            "Selected Features: ['Relative_Humidity', 'Temperature', 'Wind_Speed', 'Month', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Barisal (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Barisal_xgboost.png' and 'actual_vs_predicted_Barisal_xgboost.png'\n",
            "\n",
            "Processing data for station: Cumilla\n",
            "\n",
            "Results for Random Forest (Cumilla):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 2.0307\n",
            "R-squared Score: 0.5367\n",
            "Cross-validated R-squared Scores: [0.53608189 0.44669337 0.57738417 0.54692575 0.51586608]\n",
            "Mean CV R-squared: 0.5246\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Cumilla (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Cumilla_random_forest.png' and 'actual_vs_predicted_Cumilla_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Cumilla):\n",
            "Best Parameters: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Mean Squared Error: 1.8697\n",
            "R-squared Score: 0.5734\n",
            "Cross-validated R-squared Scores: [0.55853035 0.42267589 0.57633603 0.56456779 0.51369963]\n",
            "Mean CV R-squared: 0.5272\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Direction', 'Month', 'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Cumilla (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Cumilla_xgboost.png' and 'actual_vs_predicted_Cumilla_xgboost.png'\n",
            "\n",
            "Processing data for station: Darussalam\n",
            "\n",
            "Results for Random Forest (Darussalam):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean Squared Error: 1.8023\n",
            "R-squared Score: 0.5235\n",
            "Cross-validated R-squared Scores: [0.53399295 0.59669167 0.60068039 0.60671555 0.61734367]\n",
            "Mean CV R-squared: 0.5911\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Darussalam (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Darussalam_random_forest.png' and 'actual_vs_predicted_Darussalam_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Darussalam):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Mean Squared Error: 1.6119\n",
            "R-squared Score: 0.5738\n",
            "Cross-validated R-squared Scores: [0.57604575 0.59061269 0.58797992 0.60150369 0.59787726]\n",
            "Mean CV R-squared: 0.5908\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Temperature', 'Wind_Speed', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Darussalam (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Darussalam_xgboost.png' and 'actual_vs_predicted_Darussalam_xgboost.png'\n",
            "\n",
            "Processing data for station: DoE\n",
            "\n",
            "Results for Random Forest (DoE):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean Squared Error: 11.7703\n",
            "R-squared Score: 0.2644\n",
            "Cross-validated R-squared Scores: [0.27086667 0.18863713 0.31028569 0.19916248 0.21327981]\n",
            "Mean CV R-squared: 0.2364\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Top 3 Features:\n",
            "           Feature  Importance\n",
            "6            Month    0.164178\n",
            "0    NO2_Satellite    0.155337\n",
            "2  Solar_Radiation    0.124534\n",
            "Plots saved as 'feature_importance_DoE_random_forest.png' and 'actual_vs_predicted_DoE_random_forest.png'\n",
            "\n",
            "Results for XGBoost (DoE):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Mean Squared Error: 10.6211\n",
            "R-squared Score: 0.3362\n",
            "Cross-validated R-squared Scores: [0.28336492 0.28570936 0.37058284 0.25771887 0.22591272]\n",
            "Mean CV R-squared: 0.2847\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Top 3 Features:\n",
            "       Feature  Importance\n",
            "6        Month    0.155939\n",
            "3  Temperature    0.124005\n",
            "4   Wind_Speed    0.121700\n",
            "Plots saved as 'feature_importance_DoE_xgboost.png' and 'actual_vs_predicted_DoE_xgboost.png'\n",
            "\n",
            "Processing data for station: Gazipur\n",
            "Feature Temperature is constant for station Gazipur. Dropping.\n",
            "\n",
            "Results for Random Forest (Gazipur):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "Mean Squared Error: 0.0002\n",
            "R-squared Score: 0.8687\n",
            "Cross-validated R-squared Scores: [0.85077671 0.98037881 0.95581783 0.96361246 0.90114396]\n",
            "Mean CV R-squared: 0.9303\n",
            "Selected Features: ['NO2_Satellite', 'Solar_Radiation', 'Wind_Direction', 'Month']\n",
            "Error plotting feature importance for Gazipur (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Gazipur_random_forest.png' and 'actual_vs_predicted_Gazipur_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Gazipur):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
            "Mean Squared Error: 0.0003\n",
            "R-squared Score: 0.8077\n",
            "Cross-validated R-squared Scores: [0.80769531 0.98163898 0.96252517 0.97726585 0.94149947]\n",
            "Mean CV R-squared: 0.9341\n",
            "Selected Features: ['Solar_Radiation', 'Month']\n",
            "Error plotting feature importance for Gazipur (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Gazipur_xgboost.png' and 'actual_vs_predicted_Gazipur_xgboost.png'\n",
            "\n",
            "Processing data for station: Khulna\n",
            "\n",
            "Results for Random Forest (Khulna):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean Squared Error: 0.0628\n",
            "R-squared Score: 0.8861\n",
            "Cross-validated R-squared Scores: [0.88319386 0.92035783 0.87005877 0.86905771 0.86884645]\n",
            "Mean CV R-squared: 0.8823\n",
            "Selected Features: ['Relative_Humidity', 'Temperature', 'Wind_Speed', 'Month', 'Temp_RH_Interaction']\n",
            "Error plotting feature importance for Khulna (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Khulna_random_forest.png' and 'actual_vs_predicted_Khulna_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Khulna):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n",
            "Mean Squared Error: 0.0711\n",
            "R-squared Score: 0.8710\n",
            "Cross-validated R-squared Scores: [0.87100184 0.91182919 0.8446405  0.73207424 0.91201506]\n",
            "Mean CV R-squared: 0.8543\n",
            "Selected Features: ['Relative_Humidity', 'Month', 'Temp_RH_Interaction']\n",
            "Error plotting feature importance for Khulna (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Khulna_xgboost.png' and 'actual_vs_predicted_Khulna_xgboost.png'\n",
            "\n",
            "Processing data for station: Mymensingh\n",
            "\n",
            "Results for Random Forest (Mymensingh):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 12.1466\n",
            "R-squared Score: 0.5341\n",
            "Cross-validated R-squared Scores: [0.53691866 0.56398699 0.32129039 0.38669198 0.5366176 ]\n",
            "Mean CV R-squared: 0.4691\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Mymensingh (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Mymensingh_random_forest.png' and 'actual_vs_predicted_Mymensingh_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Mymensingh):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
            "Mean Squared Error: 12.1497\n",
            "R-squared Score: 0.5340\n",
            "Cross-validated R-squared Scores: [0.53402131 0.54155118 0.24581641 0.20289856 0.47718911]\n",
            "Mean CV R-squared: 0.4003\n",
            "Selected Features: ['Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Direction', 'Month', 'Temp_RH_Interaction']\n",
            "Error plotting feature importance for Mymensingh (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Mymensingh_xgboost.png' and 'actual_vs_predicted_Mymensingh_xgboost.png'\n",
            "\n",
            "Processing data for station: NG\n",
            "Feature Relative_Humidity is constant for station NG. Dropping.\n",
            "Feature Solar_Radiation is constant for station NG. Dropping.\n",
            "Feature Temperature is constant for station NG. Dropping.\n",
            "Feature Wind_Speed is constant for station NG. Dropping.\n",
            "Feature Wind_Direction is constant for station NG. Dropping.\n",
            "\n",
            "Results for Random Forest (NG):\n",
            "Best Parameters: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 0.3117\n",
            "R-squared Score: 0.3338\n",
            "Cross-validated R-squared Scores: [0.32764076 0.3324736  0.38600678 0.30731321 0.4340899 ]\n",
            "Mean CV R-squared: 0.3575\n",
            "Selected Features: ['NO2_Satellite', 'Month', 'DayOfWeek']\n",
            "Error plotting feature importance for NG (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_NG_random_forest.png' and 'actual_vs_predicted_NG_random_forest.png'\n",
            "\n",
            "Results for XGBoost (NG):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Mean Squared Error: 0.2722\n",
            "R-squared Score: 0.4181\n",
            "Cross-validated R-squared Scores: [0.41318785 0.28808602 0.50651486 0.29589635 0.50908392]\n",
            "Mean CV R-squared: 0.4026\n",
            "Selected Features: ['NO2_Satellite', 'Month', 'DayOfWeek']\n",
            "Error plotting feature importance for NG (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_NG_xgboost.png' and 'actual_vs_predicted_NG_xgboost.png'\n",
            "\n",
            "Processing data for station: NS\n",
            "\n",
            "Results for Random Forest (NS):\n",
            "Best Parameters: {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Mean Squared Error: 3.3018\n",
            "R-squared Score: 0.5398\n",
            "Cross-validated R-squared Scores: [0.54084457 0.59822023 0.60526261 0.54628059 0.56991773]\n",
            "Mean CV R-squared: 0.5721\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction']\n",
            "Error plotting feature importance for NS (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_NS_random_forest.png' and 'actual_vs_predicted_NS_random_forest.png'\n",
            "\n",
            "Results for XGBoost (NS):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Mean Squared Error: 2.9887\n",
            "R-squared Score: 0.5835\n",
            "Cross-validated R-squared Scores: [0.61048328 0.58344261 0.52710685 0.55898007 0.56955954]\n",
            "Mean CV R-squared: 0.5699\n",
            "Selected Features: ['Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction']\n",
            "Error plotting feature importance for NS (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_NS_xgboost.png' and 'actual_vs_predicted_NS_xgboost.png'\n",
            "\n",
            "Processing data for station: Rajshahi\n",
            "\n",
            "Results for Random Forest (Rajshahi):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "Mean Squared Error: 0.2097\n",
            "R-squared Score: 0.7711\n",
            "Cross-validated R-squared Scores: [0.7667739  0.71224474 0.71285577 0.84256038 0.70519422]\n",
            "Mean CV R-squared: 0.7479\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Rajshahi (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Rajshahi_random_forest.png' and 'actual_vs_predicted_Rajshahi_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Rajshahi):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n",
            "Mean Squared Error: 0.2438\n",
            "R-squared Score: 0.7339\n",
            "Cross-validated R-squared Scores: [0.73394079 0.77189412 0.69970709 0.79289534 0.74693827]\n",
            "Mean CV R-squared: 0.7491\n",
            "Selected Features: ['Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Month', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Rajshahi (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Rajshahi_xgboost.png' and 'actual_vs_predicted_Rajshahi_xgboost.png'\n",
            "\n",
            "Processing data for station: Rangpur\n",
            "\n",
            "Results for Random Forest (Rangpur):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mean Squared Error: 6.2761\n",
            "R-squared Score: 0.4258\n",
            "Cross-validated R-squared Scores: [0.43891135 0.50727301 0.58518015 0.42143562 0.28179376]\n",
            "Mean CV R-squared: 0.4469\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Rangpur (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Rangpur_random_forest.png' and 'actual_vs_predicted_Rangpur_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Rangpur):\n",
            "Best Parameters: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Mean Squared Error: 6.4725\n",
            "R-squared Score: 0.4078\n",
            "Cross-validated R-squared Scores: [0.42416783 0.4490511  0.52117273 0.41913726 0.34177045]\n",
            "Mean CV R-squared: 0.4311\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Rangpur (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Rangpur_xgboost.png' and 'actual_vs_predicted_Rangpur_xgboost.png'\n",
            "\n",
            "Processing data for station: Savar\n",
            "\n",
            "Results for Random Forest (Savar):\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "Mean Squared Error: 10.1919\n",
            "R-squared Score: 0.4566\n",
            "Cross-validated R-squared Scores: [0.47675748 0.39784316 0.56457551 0.5710982  0.50133747]\n",
            "Mean CV R-squared: 0.5023\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Savar (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Savar_random_forest.png' and 'actual_vs_predicted_Savar_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Savar):\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Mean Squared Error: 10.2870\n",
            "R-squared Score: 0.4515\n",
            "Cross-validated R-squared Scores: [0.43982294 0.34202684 0.55971589 0.56730283 0.4838442 ]\n",
            "Mean CV R-squared: 0.4785\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for Savar (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Savar_xgboost.png' and 'actual_vs_predicted_Savar_xgboost.png'\n",
            "\n",
            "Processing data for station: Sylhet\n",
            "Feature Temperature is constant for station Sylhet. Dropping.\n",
            "Feature Wind_Speed is constant for station Sylhet. Dropping.\n",
            "Feature Wind_Direction is constant for station Sylhet. Dropping.\n",
            "\n",
            "Results for Random Forest (Sylhet):\n",
            "Best Parameters: {'max_depth': 5, 'max_features': 0.5, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Mean Squared Error: 3.7508\n",
            "R-squared Score: 0.5130\n",
            "Cross-validated R-squared Scores: [0.51393621 0.63688928 0.64684654 0.68371201 0.60265697]\n",
            "Mean CV R-squared: 0.6168\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Month']\n",
            "Error plotting feature importance for Sylhet (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Sylhet_random_forest.png' and 'actual_vs_predicted_Sylhet_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Sylhet):\n",
            "Best Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n",
            "Mean Squared Error: 4.1894\n",
            "R-squared Score: 0.4561\n",
            "Cross-validated R-squared Scores: [0.45606123 0.65958817 0.62551343 0.66458754 0.59459541]\n",
            "Mean CV R-squared: 0.6001\n",
            "Selected Features: ['Relative_Humidity', 'Month']\n",
            "Error plotting feature importance for Sylhet (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_Sylhet_xgboost.png' and 'actual_vs_predicted_Sylhet_xgboost.png'\n",
            "\n",
            "Processing data for station: TV_Center\n",
            "Feature NO2_Satellite is constant for station TV_Center. Dropping.\n",
            "\n",
            "Results for Random Forest (TV_Center):\n",
            "Best Parameters: {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean Squared Error: 2.8110\n",
            "R-squared Score: 0.4780\n",
            "Cross-validated R-squared Scores: [0.45898549 0.47902895 0.50527623 0.51769309 0.48785576]\n",
            "Mean CV R-squared: 0.4898\n",
            "Selected Features: ['Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for TV_Center (Random Forest): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_TV_Center_random_forest.png' and 'actual_vs_predicted_TV_Center_random_forest.png'\n",
            "\n",
            "Results for XGBoost (TV_Center):\n",
            "Best Parameters: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Mean Squared Error: 2.7777\n",
            "R-squared Score: 0.4842\n",
            "Cross-validated R-squared Scores: [0.47948023 0.49523929 0.50834227 0.56058626 0.46894343]\n",
            "Mean CV R-squared: 0.5025\n",
            "Selected Features: ['Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction']\n",
            "Error plotting feature importance for TV_Center (XGBoost): All arrays must be of the same length\n",
            "Plots saved as 'feature_importance_TV_Center_xgboost.png' and 'actual_vs_predicted_TV_Center_xgboost.png'\n",
            "\n",
            "Processing data for station: Tv_Center\n",
            "Target NO2_Ground is constant for station Tv_Center. Skipping modeling.\n",
            "Skipping Tv_Center due to insufficient or invalid data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the combined data\n",
        "def load_and_preprocess_data(file_path, exclude_stations=['tv_center'], min_samples=10):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Standardize station names to lowercase\n",
        "        df['Station'] = df['Station'].str.lower()\n",
        "        exclude_stations = [s.lower() for s in exclude_stations]\n",
        "\n",
        "        # Exclude specified stations\n",
        "        df = df[~df['Station'].isin(exclude_stations)]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available after excluding stations {exclude_stations}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Check for constant or insufficient target data\n",
        "        if df['NO2_Ground'].nunique() <= 1:\n",
        "            print(f\"Target NO2_Ground is constant in combined data. Skipping modeling.\")\n",
        "            return None, None, None, None, None, None\n",
        "        if len(df) < min_samples:\n",
        "            print(f\"Insufficient samples ({len(df)}) in combined data. Minimum required: {min_samples}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Converting Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Select numeric columns for averaging\n",
        "        numeric_columns = ['NO2_Ground', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                          'Temperature', 'Wind_Speed', 'Wind_Direction', 'NO2_Satellite']\n",
        "        df_numeric = df[['Date', 'Station'] + [col for col in numeric_columns if col in df.columns]]\n",
        "\n",
        "        # Handling duplicates by averaging numeric columns per Date and Station\n",
        "        df_numeric = df_numeric.groupby(['Date', 'Station']).mean().reset_index()\n",
        "\n",
        "        # Handling missing values\n",
        "        for col in numeric_columns:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = df_numeric[col].fillna(df_numeric[col].mean())\n",
        "        if 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Direction'] = df_numeric['Wind_Direction'].fillna(df_numeric['Wind_Direction'].median())\n",
        "\n",
        "        # Scale NO2_Ground\n",
        "        target_scaler = StandardScaler()\n",
        "        df_numeric['NO2_Ground'] = target_scaler.fit_transform(df_numeric[['NO2_Ground']])\n",
        "\n",
        "        # Check for constant features and remove them\n",
        "        for col in df_numeric.columns:\n",
        "            if col not in ['Date', 'Station', 'NO2_Ground'] and df_numeric[col].nunique() <= 1:\n",
        "                print(f\"Feature {col} is constant in combined data. Dropping.\")\n",
        "                df_numeric = df_numeric.drop(columns=[col])\n",
        "\n",
        "        # Stricter outlier removal (1.5 * IQR)\n",
        "        def remove_outliers(df, column):\n",
        "            if column in df.columns and not df[column].empty:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "            return df\n",
        "\n",
        "        df_numeric = remove_outliers(df_numeric, 'NO2_Ground')\n",
        "\n",
        "        # Feature engineering\n",
        "        df_numeric['Month'] = df_numeric['Date'].dt.month\n",
        "        df_numeric['DayOfWeek'] = df_numeric['Date'].dt.dayofweek\n",
        "        if 'Temperature' in df_numeric.columns and 'Relative_Humidity' in df_numeric.columns:\n",
        "            df_numeric['Temp_RH_Interaction'] = df_numeric['Temperature'] * df_numeric['Relative_Humidity']\n",
        "        else:\n",
        "            df_numeric['Temp_RH_Interaction'] = 0\n",
        "        if 'Wind_Speed' in df_numeric.columns and 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Speed_Direction'] = df_numeric['Wind_Speed'] * np.cos(np.radians(df_numeric['Wind_Direction']))\n",
        "        else:\n",
        "            df_numeric['Wind_Speed_Direction'] = 0\n",
        "\n",
        "        # Selecting features and target\n",
        "        features = ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                    'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month',\n",
        "                    'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction', 'Station']\n",
        "        features = [f for f in features if f in df_numeric.columns]\n",
        "        if not features or 'NO2_Ground' not in df_numeric.columns:\n",
        "            print(f\"Insufficient features or target in combined data.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Correlation analysis\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            correlation = df_numeric[['NO2_Satellite', 'NO2_Ground']].corr().iloc[0, 1]\n",
        "            print(f\"Correlation between NO2_Satellite and NO2_Ground in combined data: {correlation:.4f}\")\n",
        "\n",
        "        X = df_numeric[features]\n",
        "        y = df_numeric['NO2_Ground']\n",
        "\n",
        "        # Define preprocessing pipeline\n",
        "        numeric_features = [f for f in features if f != 'Station']\n",
        "        categorical_features = ['Station'] if 'Station' in features else []\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', StandardScaler(), numeric_features),\n",
        "                ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
        "            ])\n",
        "\n",
        "        return X, y, df_numeric, features, preprocessor, target_scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing combined data: {str(e)}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "# Training Random Forest model\n",
        "def train_random_forest(X, y, preprocessor):\n",
        "    try:\n",
        "        # Splitting the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Define pipeline\n",
        "        rf_param_grid = {\n",
        "            'rf__n_estimators': [100, 200],\n",
        "            'rf__max_depth': [5, 10, 15],\n",
        "            'rf__min_samples_split': [2, 5],\n",
        "            'rf__min_samples_leaf': [1, 2],\n",
        "            'rf__max_features': ['sqrt', 0.5]\n",
        "        }\n",
        "        rf_model = RandomForestRegressor(random_state=42)\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('rf', rf_model)\n",
        "        ])\n",
        "\n",
        "        # Grid search with KFold\n",
        "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        grid_search = GridSearchCV(pipeline, rf_param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Best model\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        # Metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        cv_scores = cross_val_score(best_model, X, y, cv=kfold, scoring='r2')\n",
        "\n",
        "        return best_model, X_train, X_test, y_train, y_test, y_pred, mse, r2, cv_scores, grid_search\n",
        "    except Exception as e:\n",
        "        print(f\"Error training Random Forest model: {str(e)}\")\n",
        "        return None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, features, numeric_features, categorical_features):\n",
        "    try:\n",
        "        # Extract feature importances\n",
        "        importance = model.named_steps['rf'].feature_importances_\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = numeric_features\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = num_features + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "        plt.title('Feature Importance in Random Forest Model (Combined Stations)')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_importance_combined_stations.png')\n",
        "        plt.close()\n",
        "\n",
        "        return feature_importance.head(3)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting feature importance: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred):\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual NO2 Ground Concentration (Scaled)')\n",
        "        plt.ylabel('Predicted NO2 Ground Concentration (Scaled)')\n",
        "        plt.title('Actual vs Predicted NO2 Ground Concentrations (Combined Stations)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('actual_vs_predicted_combined_stations.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting actual vs predicted: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    result = load_and_preprocess_data(file_path, exclude_stations=['TV_Center', 'Tv_Center'])\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to preprocess data. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X, y, df, features, preprocessor, target_scaler = result\n",
        "    print(f\"Combined data from stations: {df['Station'].unique().tolist()}\")\n",
        "\n",
        "    # Train Random Forest\n",
        "    print(\"Training Random Forest model...\")\n",
        "    result = train_random_forest(X, y, preprocessor)\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to train model. Exiting.\")\n",
        "        return\n",
        "\n",
        "    best_model, X_train, X_test, y_train, y_test, y_pred, mse, r2, cv_scores, grid_search = result\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nResults for Random Forest (Combined Stations):\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"R-squared Score: {r2:.4f}\")\n",
        "    print(f\"Cross-validated R-squared Scores: {cv_scores}\")\n",
        "    print(f\"Mean CV R-squared: {cv_scores.mean():.4f}\")\n",
        "\n",
        "    # Plot feature importance\n",
        "    numeric_features = [f for f in features if f != 'Station']\n",
        "    categorical_features = ['Station'] if 'Station' in features else []\n",
        "    top_features = plot_feature_importance(best_model, features, numeric_features, categorical_features)\n",
        "    if top_features is not None:\n",
        "        print(f\"Top 3 Features:\\n{top_features}\")\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    plot_actual_vs_predicted(y_test, y_pred)\n",
        "\n",
        "    print(\"Plots saved as 'feature_importance_combined_stations.png' and 'actual_vs_predicted_combined_stations.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AMc9E0A8S7H",
        "outputId": "5d4d50bb-dfec-4b2b-8647-64f1c4c5162a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Correlation between NO2_Satellite and NO2_Ground in combined data: -0.0063\n",
            "Combined data from stations: ['agrabad', 'barisal', 'cumilla', 'darussalam', 'doe', 'gazipur', 'khulna', 'mymensingh', 'ng', 'ns', 'rangpur', 'savar', 'sylhet', 'rajshahi', 'brac']\n",
            "Training Random Forest model...\n",
            "\n",
            "Results for Random Forest (Combined Stations):\n",
            "Best Parameters: {'rf__max_depth': 15, 'rf__max_features': 0.5, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 5, 'rf__n_estimators': 200}\n",
            "Mean Squared Error: 0.0080\n",
            "R-squared Score: 0.6292\n",
            "Cross-validated R-squared Scores: [0.63099482 0.62084351 0.60657378 0.62064993 0.63040297]\n",
            "Mean CV R-squared: 0.6219\n",
            "Top 3 Features:\n",
            "            Feature  Importance\n",
            "10  Station_agrabad    0.127563\n",
            "2   Solar_Radiation    0.112948\n",
            "6             Month    0.106490\n",
            "Plots saved as 'feature_importance_combined_stations.png' and 'actual_vs_predicted_combined_stations.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the combined data\n",
        "def load_and_preprocess_data(file_path, exclude_stations=['tv_center'], min_samples=10):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Standardize station names to lowercase\n",
        "        df['Station'] = df['Station'].str.lower()\n",
        "        exclude_stations = [s.lower() for s in exclude_stations]\n",
        "\n",
        "        # Exclude specified stations\n",
        "        df = df[~df['Station'].isin(exclude_stations)]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available after excluding stations {exclude_stations}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Check for constant or insufficient target data\n",
        "        if df['NO2_Ground'].nunique() <= 1:\n",
        "            print(f\"Target NO2_Ground is constant in combined data. Skipping modeling.\")\n",
        "            return None, None, None, None, None, None\n",
        "        if len(df) < min_samples:\n",
        "            print(f\"Insufficient samples ({len(df)}) in combined data. Minimum required: {min_samples}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Converting Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Select numeric columns for averaging\n",
        "        numeric_columns = ['NO2_Ground', 'NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                          'Temperature', 'Wind_Speed', 'Wind_Direction']\n",
        "        df_numeric = df[['Date', 'Station'] + [col for col in numeric_columns if col in df.columns]]\n",
        "\n",
        "        # Handling duplicates by averaging numeric columns per Date and Station\n",
        "        df_numeric = df_numeric.groupby(['Date', 'Station']).mean().reset_index()\n",
        "\n",
        "        # Handling missing values\n",
        "        for col in numeric_columns:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = df_numeric[col].fillna(df_numeric[col].mean())\n",
        "        if 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Direction'] = df_numeric['Wind_Direction'].fillna(df_numeric['Wind_Direction'].median())\n",
        "\n",
        "        # Log-transform skewed features\n",
        "        for col in ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Wind_Speed']:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = np.log1p(df_numeric[col].clip(lower=0))  # Avoid log(0)\n",
        "\n",
        "        # Scale NO2_Ground\n",
        "        target_scaler = StandardScaler()\n",
        "        df_numeric['NO2_Ground'] = target_scaler.fit_transform(df_numeric[['NO2_Ground']])\n",
        "\n",
        "        # Check for constant features and remove them\n",
        "        for col in df_numeric.columns:\n",
        "            if col not in ['Date', 'Station', 'NO2_Ground'] and df_numeric[col].nunique() <= 1:\n",
        "                print(f\"Feature {col} is constant in combined data. Dropping.\")\n",
        "                df_numeric = df_numeric.drop(columns=[col])\n",
        "\n",
        "        # Stricter outlier removal (1.5 * IQR)\n",
        "        def remove_outliers(df, column):\n",
        "            if column in df.columns and not df[column].empty:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "            return df\n",
        "\n",
        "        df_numeric = remove_outliers(df_numeric, 'NO2_Ground')\n",
        "\n",
        "        # Feature engineering\n",
        "        df_numeric['Month'] = df_numeric['Date'].dt.month\n",
        "        df_numeric['DayOfWeek'] = df_numeric['Date'].dt.dayofweek\n",
        "        if 'Temperature' in df_numeric.columns and 'Relative_Humidity' in df_numeric.columns:\n",
        "            df_numeric['Temp_RH_Interaction'] = df_numeric['Temperature'] * df_numeric['Relative_Humidity']\n",
        "        else:\n",
        "            df_numeric['Temp_RH_Interaction'] = 0\n",
        "        if 'Wind_Speed' in df_numeric.columns and 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Speed_Direction'] = df_numeric['Wind_Speed'] * np.cos(np.radians(df_numeric['Wind_Direction']))\n",
        "        else:\n",
        "            df_numeric['Wind_Speed_Direction'] = 0\n",
        "        if 'NO2_Satellite' in df_numeric.columns and 'Solar_Radiation' in df_numeric.columns:\n",
        "            df_numeric['Satellite_Solar_Interaction'] = df_numeric['NO2_Satellite'] * df_numeric['Solar_Radiation']\n",
        "        else:\n",
        "            df_numeric['Satellite_Solar_Interaction'] = 0\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            df_numeric['Satellite_Month_Interaction'] = df_numeric['NO2_Satellite'] * df_numeric['Month']\n",
        "        else:\n",
        "            df_numeric['Satellite_Month_Interaction'] = 0\n",
        "\n",
        "        # Station clustering\n",
        "        cluster_features = ['NO2_Ground', 'NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature']\n",
        "        cluster_data = df_numeric[[f for f in cluster_features if f in df_numeric.columns]].fillna(0)\n",
        "        if len(cluster_data) > 0:\n",
        "            kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "            df_numeric['Cluster'] = kmeans.fit_predict(cluster_data)\n",
        "        else:\n",
        "            df_numeric['Cluster'] = 0\n",
        "\n",
        "        # Selecting features and target\n",
        "        features = ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature',\n",
        "                    'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek', 'Temp_RH_Interaction',\n",
        "                    'Wind_Speed_Direction', 'Satellite_Solar_Interaction', 'Satellite_Month_Interaction',\n",
        "                    'Station', 'Cluster']\n",
        "        features = [f for f in features if f in df_numeric.columns]\n",
        "        if not features or 'NO2_Ground' not in df_numeric.columns:\n",
        "            print(f\"Insufficient features or target in combined data.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Correlation analysis\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            correlation = df_numeric[['NO2_Satellite', 'NO2_Ground']].corr().iloc[0, 1]\n",
        "            print(f\"Correlation between NO2_Satellite and NO2_Ground in combined data (after log-transform): {correlation:.4f}\")\n",
        "\n",
        "        X = df_numeric[features]\n",
        "        y = df_numeric['NO2_Ground']\n",
        "\n",
        "        # Define preprocessing pipeline\n",
        "        numeric_features = [f for f in features if f not in ['Station', 'Cluster']]\n",
        "        categorical_features = [f for f in ['Station', 'Cluster'] if f in features]\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', StandardScaler(), numeric_features),\n",
        "                ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
        "            ])\n",
        "\n",
        "        return X, y, df_numeric, features, preprocessor, target_scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing combined data: {str(e)}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "# Training and comparing models\n",
        "def train_models(X, y, preprocessor, df):\n",
        "    try:\n",
        "        # Splitting the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Get station labels for test set\n",
        "        test_indices = X_test.index\n",
        "        stations_test = df.loc[test_indices, 'Station']\n",
        "\n",
        "        # Define models and pipelines\n",
        "        models = {\n",
        "            'Random Forest': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('rf', RandomForestRegressor(random_state=42))\n",
        "            ]),\n",
        "            'XGBoost': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('xgb', XGBRegressor(random_state=42))\n",
        "            ]),\n",
        "            'Linear Regression': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('lr', LinearRegression())\n",
        "            ])\n",
        "        }\n",
        "\n",
        "        # Parameter grids\n",
        "        rf_param_grid = {\n",
        "            'rf__n_estimators': [100, 200],\n",
        "            'rf__max_depth': [5, 10],\n",
        "            'rf__min_samples_split': [2, 5],\n",
        "            'rf__min_samples_leaf': [1, 2],\n",
        "            'rf__max_features': ['sqrt', 0.5]\n",
        "        }\n",
        "        xgb_param_grid = {\n",
        "            'xgb__n_estimators': [100, 200],\n",
        "            'xgb__max_depth': [3, 5],\n",
        "            'xgb__learning_rate': [0.01, 0.1],\n",
        "            'xgb__subsample': [0.8, 1.0]\n",
        "        }\n",
        "\n",
        "        # Grid search for Random Forest and XGBoost\n",
        "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        results = {}\n",
        "        for name, pipeline in models.items():\n",
        "            if name in ['Random Forest', 'XGBoost']:\n",
        "                param_grid = rf_param_grid if name == 'Random Forest' else xgb_param_grid\n",
        "                grid_search = GridSearchCV(pipeline, param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "                grid_search.fit(X_train, y_train)\n",
        "                best_model = grid_search.best_estimator_\n",
        "                best_params = grid_search.best_params_\n",
        "            else:  # Linear Regression\n",
        "                pipeline.fit(X_train, y_train)\n",
        "                best_model = pipeline\n",
        "                best_params = None\n",
        "\n",
        "            # Predictions and metrics\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            cv_scores = cross_val_score(best_model, X, y, cv=kfold, scoring='r2')\n",
        "\n",
        "            # Per-station performance\n",
        "            station_metrics = {}\n",
        "            for station in stations_test.unique():\n",
        "                mask = stations_test == station\n",
        "                if mask.sum() > 0:\n",
        "                    station_mse = mean_squared_error(y_test[mask], y_pred[mask])\n",
        "                    station_r2 = r2_score(y_test[mask], y_pred[mask])\n",
        "                    station_metrics[station] = {'MSE': station_mse, 'R2': station_r2}\n",
        "\n",
        "            results[name] = {\n",
        "                'model': best_model,\n",
        "                'y_pred': y_pred,\n",
        "                'mse': mse,\n",
        "                'r2': r2,\n",
        "                'cv_scores': cv_scores,\n",
        "                'best_params': best_params,\n",
        "                'station_metrics': station_metrics\n",
        "            }\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, results\n",
        "    except Exception as e:\n",
        "        print(f\"Error training models: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Feature selection based on importance\n",
        "def select_important_features(model, name, features, numeric_features, categorical_features, X, threshold=0.1):\n",
        "    try:\n",
        "        if name == 'Linear Regression':\n",
        "            importance = np.abs(model.named_steps['lr'].coef_)\n",
        "        else:\n",
        "            importance = model.named_steps[name.lower().replace(' ', '_')].feature_importances_\n",
        "\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = numeric_features\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = num_features + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        selected_features = feature_importance[feature_importance['Importance'] >= threshold]['Feature'].tolist()\n",
        "\n",
        "        if not selected_features:\n",
        "            return num_features + cat_feature_names, X\n",
        "\n",
        "        # Map back to original feature names\n",
        "        selected_orig_features = []\n",
        "        for f in selected_features:\n",
        "            if f in num_features:\n",
        "                selected_orig_features.append(f)\n",
        "            else:\n",
        "                # Handle categorical features\n",
        "                for cat_feature in categorical_features:\n",
        "                    if f.startswith(cat_feature):\n",
        "                        if cat_feature not in selected_orig_features:\n",
        "                            selected_orig_features.append(cat_feature)\n",
        "\n",
        "        # Select corresponding columns in X\n",
        "        selected_cols = []\n",
        "        for f in selected_orig_features:\n",
        "            if f in numeric_features or f in categorical_features:\n",
        "                selected_cols.append(f)\n",
        "\n",
        "        return selected_cols, X[selected_cols]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature selection for {name}: {str(e)}\")\n",
        "        return features, X\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, name, features, numeric_features, categorical_features):\n",
        "    try:\n",
        "        if name == 'Linear Regression':\n",
        "            importance = np.abs(model.named_steps['lr'].coef_)\n",
        "        else:\n",
        "            importance = model.named_steps[name.lower().replace(' ', '_')].feature_importances_\n",
        "\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = numeric_features\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = num_features + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "        plt.title(f'Feature Importance in {name} Model (Combined Stations)')\n",
        "        plt.xlabel('Importance' if name != 'Linear Regression' else 'Absolute Coefficient Value')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'feature_importance_combined_stations_{name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "        return feature_importance.head(3)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting feature importance for {name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred, name):\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual NO2 Ground Concentration (Scaled)')\n",
        "        plt.ylabel('Predicted NO2 Ground Concentration (Scaled)')\n",
        "        plt.title(f'Actual vs Predicted NO2 Ground Concentrations ({name}, Combined Stations)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'actual_vs_predicted_combined_stations_{name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting actual vs predicted for {name}: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    result = load_and_preprocess_data(file_path, exclude_stations=['TV_Center', 'Tv_Center'])\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to preprocess data. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X, y, df, features, preprocessor, target_scaler = result\n",
        "    print(f\"Combined data from stations: {df['Station'].unique().tolist()}\")\n",
        "\n",
        "    # Train models\n",
        "    print(\"Training models...\")\n",
        "    result = train_models(X, y, preprocessor, df)\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to train models. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X_train, X_test, y_train, y_test, results = result\n",
        "\n",
        "    # Process each model\n",
        "    numeric_features = [f for f in features if f not in ['Station', 'Cluster']]\n",
        "    categorical_features = [f for f in ['Station', 'Cluster'] if f in features]\n",
        "\n",
        "    for name, res in results.items():\n",
        "        print(f\"\\nResults for {name} (Combined Stations):\")\n",
        "        if res['best_params']:\n",
        "            print(f\"Best Parameters: {res['best_params']}\")\n",
        "        print(f\"Mean Squared Error: {res['mse']:.4f}\")\n",
        "        print(f\"R-squared Score: {res['r2']:.4f}\")\n",
        "        print(f\"Cross-validated R-squared Scores: {res['cv_scores']}\")\n",
        "        print(f\"Mean CV R-squared: {res['cv_scores'].mean():.4f}\")\n",
        "\n",
        "        # Feature selection\n",
        "        selected_features, X_selected = select_important_features(\n",
        "            res['model'], name, features, numeric_features, categorical_features, X, threshold=0.1\n",
        "        )\n",
        "        print(f\"Selected Features: {selected_features}\")\n",
        "\n",
        "        # Plot feature importance\n",
        "        top_features = plot_feature_importance(res['model'], name, features, numeric_features, categorical_features)\n",
        "        if top_features is not None:\n",
        "            print(f\"Top 3 Features:\\n{top_features}\")\n",
        "\n",
        "        # Plot actual vs predicted\n",
        "        plot_actual_vs_predicted(y_test, res['y_pred'], name)\n",
        "\n",
        "        # Per-station performance\n",
        "        print(f\"\\nPer-Station Performance for {name}:\")\n",
        "        for station, metrics in res['station_metrics'].items():\n",
        "            print(f\"Station {station}: MSE = {metrics['MSE']:.4f}, R2 = {metrics['R2']:.4f}\")\n",
        "\n",
        "        print(f\"Plots saved as 'feature_importance_combined_stations_{name.lower().replace(' ', '_')}.png' and 'actual_vs_predicted_combined_stations_{name.lower().replace(' ', '_')}.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSf3uwG4AkCw",
        "outputId": "f5e51184-8da5-4375-9512-e69a4416d3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Correlation between NO2_Satellite and NO2_Ground in combined data (after log-transform): -0.0063\n",
            "Combined data from stations: ['agrabad', 'barisal', 'cumilla', 'darussalam', 'doe', 'gazipur', 'khulna', 'mymensingh', 'ng', 'ns', 'rangpur', 'savar', 'sylhet', 'rajshahi', 'brac']\n",
            "Training models...\n",
            "\n",
            "Results for Random Forest (Combined Stations):\n",
            "Best Parameters: {'rf__max_depth': 10, 'rf__max_features': 0.5, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 200}\n",
            "Mean Squared Error: 0.0093\n",
            "R-squared Score: 0.5706\n",
            "Cross-validated R-squared Scores: [0.56589439 0.54841128 0.54188049 0.55691856 0.55873139]\n",
            "Mean CV R-squared: 0.5544\n",
            "Error in feature selection for Random Forest: 'random_forest'\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction', 'Satellite_Solar_Interaction', 'Satellite_Month_Interaction', 'Station', 'Cluster']\n",
            "Error plotting feature importance for Random Forest: 'random_forest'\n",
            "\n",
            "Per-Station Performance for Random Forest:\n",
            "Station doe: MSE = 0.0170, R2 = 0.0927\n",
            "Station rajshahi: MSE = 0.0117, R2 = 0.2443\n",
            "Station darussalam: MSE = 0.0051, R2 = 0.2167\n",
            "Station cumilla: MSE = 0.0057, R2 = 0.3710\n",
            "Station ns: MSE = 0.0091, R2 = 0.3150\n",
            "Station rangpur: MSE = 0.0192, R2 = 0.1272\n",
            "Station mymensingh: MSE = 0.0165, R2 = 0.1657\n",
            "Station savar: MSE = 0.0133, R2 = 0.3995\n",
            "Station khulna: MSE = 0.0003, R2 = 0.6919\n",
            "Station ng: MSE = 0.0030, R2 = 0.4730\n",
            "Station barisal: MSE = 0.0066, R2 = 0.4395\n",
            "Station gazipur: MSE = 0.0002, R2 = 0.7512\n",
            "Station agrabad: MSE = 0.0116, R2 = 0.5983\n",
            "Station sylhet: MSE = 0.0125, R2 = 0.3882\n",
            "Station brac: MSE = 0.0397, R2 = 0.0719\n",
            "Plots saved as 'feature_importance_combined_stations_random_forest.png' and 'actual_vs_predicted_combined_stations_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Combined Stations):\n",
            "Best Parameters: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200, 'xgb__subsample': 0.8}\n",
            "Mean Squared Error: 0.0072\n",
            "R-squared Score: 0.6656\n",
            "Cross-validated R-squared Scores: [0.66359562 0.66363061 0.6394952  0.68038137 0.67498062]\n",
            "Mean CV R-squared: 0.6644\n",
            "Error in feature selection for XGBoost: 'xgboost'\n",
            "Selected Features: ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek', 'Temp_RH_Interaction', 'Wind_Speed_Direction', 'Satellite_Solar_Interaction', 'Satellite_Month_Interaction', 'Station', 'Cluster']\n",
            "Error plotting feature importance for XGBoost: 'xgboost'\n",
            "\n",
            "Per-Station Performance for XGBoost:\n",
            "Station doe: MSE = 0.0130, R2 = 0.3046\n",
            "Station rajshahi: MSE = 0.0076, R2 = 0.5129\n",
            "Station darussalam: MSE = 0.0038, R2 = 0.4051\n",
            "Station cumilla: MSE = 0.0053, R2 = 0.4123\n",
            "Station ns: MSE = 0.0066, R2 = 0.5036\n",
            "Station rangpur: MSE = 0.0121, R2 = 0.4483\n",
            "Station mymensingh: MSE = 0.0110, R2 = 0.4441\n",
            "Station savar: MSE = 0.0117, R2 = 0.4724\n",
            "Station khulna: MSE = 0.0005, R2 = 0.4823\n",
            "Station ng: MSE = 0.0038, R2 = 0.3312\n",
            "Station barisal: MSE = 0.0062, R2 = 0.4777\n",
            "Station gazipur: MSE = 0.0002, R2 = 0.7048\n",
            "Station agrabad: MSE = 0.0119, R2 = 0.5850\n",
            "Station sylhet: MSE = 0.0097, R2 = 0.5238\n",
            "Station brac: MSE = 0.0133, R2 = 0.6887\n",
            "Plots saved as 'feature_importance_combined_stations_xgboost.png' and 'actual_vs_predicted_combined_stations_xgboost.png'\n",
            "\n",
            "Results for Linear Regression (Combined Stations):\n",
            "Mean Squared Error: 0.0136\n",
            "R-squared Score: 0.3699\n",
            "Cross-validated R-squared Scores: [0.36994752 0.37653168 0.37968771 0.40783526 0.36023019]\n",
            "Mean CV R-squared: 0.3788\n",
            "Selected Features: ['Station']\n",
            "Top 3 Features:\n",
            "            Feature  Importance\n",
            "14     Station_brac    0.202795\n",
            "12  Station_agrabad    0.187433\n",
            "21       Station_ng    0.154538\n",
            "\n",
            "Per-Station Performance for Linear Regression:\n",
            "Station doe: MSE = 0.0184, R2 = 0.0162\n",
            "Station rajshahi: MSE = 0.0144, R2 = 0.0701\n",
            "Station darussalam: MSE = 0.0051, R2 = 0.2030\n",
            "Station cumilla: MSE = 0.0097, R2 = -0.0788\n",
            "Station ns: MSE = 0.0144, R2 = -0.0878\n",
            "Station rangpur: MSE = 0.0236, R2 = -0.0734\n",
            "Station mymensingh: MSE = 0.0204, R2 = -0.0275\n",
            "Station savar: MSE = 0.0221, R2 = 0.0030\n",
            "Station khulna: MSE = 0.0018, R2 = -1.0737\n",
            "Station ng: MSE = 0.0068, R2 = -0.1873\n",
            "Station barisal: MSE = 0.0131, R2 = -0.1121\n",
            "Station gazipur: MSE = 0.0010, R2 = -0.2943\n",
            "Station agrabad: MSE = 0.0248, R2 = 0.1382\n",
            "Station sylhet: MSE = 0.0164, R2 = 0.1952\n",
            "Station brac: MSE = 0.0506, R2 = -0.1807\n",
            "Plots saved as 'feature_importance_combined_stations_linear_regression.png' and 'actual_vs_predicted_combined_stations_linear_regression.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the combined data\n",
        "def load_and_preprocess_data(file_path, exclude_stations=['tv_center'], min_samples=10):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Standardize station names to lowercase\n",
        "        df['Station'] = df['Station'].str.lower()\n",
        "        exclude_stations = [s.lower() for s in exclude_stations]\n",
        "\n",
        "        # Exclude specified stations\n",
        "        df = df[~df['Station'].isin(exclude_stations)]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available after excluding stations {exclude_stations}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Check for constant or insufficient target data\n",
        "        if df['NO2_Ground'].nunique() <= 1:\n",
        "            print(f\"Target NO2_Ground is constant in combined data. Skipping modeling.\")\n",
        "            return None, None, None, None, None, None\n",
        "        if len(df) < min_samples:\n",
        "            print(f\"Insufficient samples ({len(df)}) in combined data. Minimum required: {min_samples}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Converting Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Select numeric columns for averaging\n",
        "        numeric_columns = ['NO2_Ground', 'NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                          'Temperature', 'Wind_Speed', 'Wind_Direction']\n",
        "        df_numeric = df[['Date', 'Station'] + [col for col in numeric_columns if col in df.columns]]\n",
        "\n",
        "        # Handling duplicates by averaging numeric columns per Date and Station\n",
        "        df_numeric = df_numeric.groupby(['Date', 'Station']).mean().reset_index()\n",
        "\n",
        "        # Station-specific scaling for NO2_Satellite\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            for station in df_numeric['Station'].unique():\n",
        "                mask = df_numeric['Station'] == station\n",
        "                if mask.sum() > 0:\n",
        "                    scaler = StandardScaler()\n",
        "                    df_numeric.loc[mask, 'NO2_Satellite'] = scaler.fit_transform(\n",
        "                        df_numeric.loc[mask, ['NO2_Satellite']]\n",
        "                    )\n",
        "\n",
        "        # Handling missing values\n",
        "        for col in numeric_columns:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = df_numeric[col].fillna(df_numeric[col].mean())\n",
        "        if 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Direction'] = df_numeric['Wind_Direction'].fillna(df_numeric['Wind_Direction'].median())\n",
        "\n",
        "        # Log-transform skewed features\n",
        "        for col in ['Relative_Humidity', 'Solar_Radiation', 'Wind_Speed']:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = np.log1p(df_numeric[col].clip(lower=0))  # Avoid log(0)\n",
        "\n",
        "        # Scale NO2_Ground\n",
        "        target_scaler = StandardScaler()\n",
        "        df_numeric['NO2_Ground'] = target_scaler.fit_transform(df_numeric[['NO2_Ground']])\n",
        "\n",
        "        # Check for constant features and remove them\n",
        "        for col in df_numeric.columns:\n",
        "            if col not in ['Date', 'Station', 'NO2_Ground'] and df_numeric[col].nunique() <= 1:\n",
        "                print(f\"Feature {col} is constant in combined data. Dropping.\")\n",
        "                df_numeric = df_numeric.drop(columns=[col])\n",
        "\n",
        "        # Stricter outlier removal (1.5 * IQR)\n",
        "        def remove_outliers(df, column):\n",
        "            if column in df.columns and not df[column].empty:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "            return df\n",
        "\n",
        "        df_numeric = remove_outliers(df_numeric, 'NO2_Ground')\n",
        "\n",
        "        # Feature engineering\n",
        "        df_numeric['Month'] = df_numeric['Date'].dt.month\n",
        "        df_numeric['DayOfWeek'] = df_numeric['Date'].dt.dayofweek\n",
        "        if 'Temperature' in df_numeric.columns and 'Relative_Humidity' in df_numeric.columns:\n",
        "            df_numeric['Temp_RH_Interaction'] = df_numeric['Temperature'] * df_numeric['Relative_Humidity']\n",
        "        else:\n",
        "            df_numeric['Temp_RH_Interaction'] = 0\n",
        "        if 'Wind_Speed' in df_numeric.columns and 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Speed_Direction'] = df_numeric['Wind_Speed'] * np.cos(np.radians(df_numeric['Wind_Direction']))\n",
        "        else:\n",
        "            df_numeric['Wind_Speed_Direction'] = 0\n",
        "        if 'NO2_Satellite' in df_numeric.columns and 'Solar_Radiation' in df_numeric.columns:\n",
        "            df_numeric['Satellite_Solar_Interaction'] = df_numeric['NO2_Satellite'] * df_numeric['Solar_Radiation']\n",
        "        else:\n",
        "            df_numeric['Satellite_Solar_Interaction'] = 0\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            df_numeric['Satellite_Month_Interaction'] = df_numeric['NO2_Satellite'] * df_numeric['Month']\n",
        "        else:\n",
        "            df_numeric['Satellite_Month_Interaction'] = 0\n",
        "\n",
        "        # Station clustering\n",
        "        cluster_features = ['NO2_Ground', 'NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature']\n",
        "        cluster_data = df_numeric[[f for f in cluster_features if f in df_numeric.columns]].fillna(0)\n",
        "        if len(cluster_data) > 0:\n",
        "            kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "            df_numeric['Cluster'] = kmeans.fit_predict(cluster_data)\n",
        "        else:\n",
        "            df_numeric['Cluster'] = 0\n",
        "\n",
        "        # Selecting features and target\n",
        "        features = ['NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature',\n",
        "                    'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek', 'Temp_RH_Interaction',\n",
        "                    'Wind_Speed_Direction', 'Satellite_Solar_Interaction', 'Satellite_Month_Interaction',\n",
        "                    'Station', 'Cluster']\n",
        "        features = [f for f in features if f in df_numeric.columns]\n",
        "        if not features or 'NO2_Ground' not in df_numeric.columns:\n",
        "            print(f\"Insufficient features or target in combined data.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Correlation analysis\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            correlation = df_numeric[['NO2_Satellite', 'NO2_Ground']].corr().iloc[0, 1]\n",
        "            print(f\"Correlation between NO2_Satellite and NO2_Ground in combined data (after transformations): {correlation:.4f}\")\n",
        "\n",
        "        X = df_numeric[features]\n",
        "        y = df_numeric['NO2_Ground']\n",
        "\n",
        "        # Define preprocessing pipeline with polynomial features\n",
        "        numeric_features = [f for f in features if f not in ['Station', 'Cluster']]\n",
        "        categorical_features = [f for f in ['Station', 'Cluster'] if f in features]\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', Pipeline([\n",
        "                    ('scaler', StandardScaler()),\n",
        "                    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n",
        "                ]), numeric_features),\n",
        "                ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
        "            ])\n",
        "\n",
        "        return X, y, df_numeric, features, preprocessor, target_scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing combined data: {str(e)}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "# Training and comparing models\n",
        "def train_models(X, y, preprocessor, df):\n",
        "    try:\n",
        "        # Splitting the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Get station labels for test set\n",
        "        test_indices = X_test.index\n",
        "        stations_test = df.loc[test_indices, 'Station']\n",
        "\n",
        "        # Define models and pipelines\n",
        "        models = {\n",
        "            'Random Forest': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('rf', RandomForestRegressor(random_state=42))\n",
        "            ]),\n",
        "            'XGBoost': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('xgb', XGBRegressor(random_state=42))\n",
        "            ])\n",
        "        }\n",
        "\n",
        "        # Parameter grids\n",
        "        rf_param_grid = {\n",
        "            'rf__n_estimators': [100, 200, 300],\n",
        "            'rf__max_depth': [5, 10, 15],\n",
        "            'rf__min_samples_split': [2, 5],\n",
        "            'rf__min_samples_leaf': [1, 2],\n",
        "            'rf__max_features': ['sqrt', 0.5]\n",
        "        }\n",
        "        xgb_param_grid = {\n",
        "            'xgb__n_estimators': [100, 200, 300],\n",
        "            'xgb__max_depth': [3, 5, 7],\n",
        "            'xgb__learning_rate': [0.01, 0.1],\n",
        "            'xgb__subsample': [0.8, 1.0]\n",
        "        }\n",
        "\n",
        "        # Grid search for Random Forest and XGBoost\n",
        "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        results = {}\n",
        "        for name, pipeline in models.items():\n",
        "            param_grid = rf_param_grid if name == 'Random Forest' else xgb_param_grid\n",
        "            grid_search = GridSearchCV(pipeline, param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_model = grid_search.best_estimator_\n",
        "            best_params = grid_search.best_params_\n",
        "\n",
        "            # Predictions and metrics\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            cv_scores = cross_val_score(best_model, X, y, cv=kfold, scoring='r2')\n",
        "\n",
        "            # Per-station performance\n",
        "            station_metrics = {}\n",
        "            for station in stations_test.unique():\n",
        "                mask = stations_test == station\n",
        "                if mask.sum() > 0:\n",
        "                    station_mse = mean_squared_error(y_test[mask], y_pred[mask])\n",
        "                    station_r2 = r2_score(y_test[mask], y_pred[mask])\n",
        "                    station_metrics[station] = {'MSE': station_mse, 'R2': station_r2}\n",
        "\n",
        "            results[name] = {\n",
        "                'model': best_model,\n",
        "                'y_pred': y_pred,\n",
        "                'mse': mse,\n",
        "                'r2': r2,\n",
        "                'cv_scores': cv_scores,\n",
        "                'best_params': best_params,\n",
        "                'station_metrics': station_metrics\n",
        "            }\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, results\n",
        "    except Exception as e:\n",
        "        print(f\"Error training models: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Feature selection based on importance\n",
        "def select_important_features(model, name, features, numeric_features, categorical_features, X, threshold=0.05):\n",
        "    try:\n",
        "        if name == 'Random Forest':\n",
        "            importance = model.named_steps['rf'].feature_importances_\n",
        "        else:  # XGBoost\n",
        "            importance = model.named_steps['xgb'].feature_importances_\n",
        "\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = []\n",
        "        poly = preprocessor.named_transformers_['num'].named_steps['poly']\n",
        "        poly_features = poly.get_feature_names_out(numeric_features)\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = list(poly_features) + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        selected_features = feature_importance[feature_importance['Importance'] >= threshold]['Feature'].tolist()\n",
        "\n",
        "        if not selected_features:\n",
        "            return features, X\n",
        "\n",
        "        # Map back to original feature names\n",
        "        selected_orig_features = []\n",
        "        for f in selected_features:\n",
        "            if f in poly_features:\n",
        "                # Extract original numeric features from polynomial terms\n",
        "                for orig_f in numeric_features:\n",
        "                    if orig_f in f and orig_f not in selected_orig_features:\n",
        "                        selected_orig_features.append(orig_f)\n",
        "            elif f in cat_feature_names:\n",
        "                for cat_feature in categorical_features:\n",
        "                    if f.startswith(cat_feature) and cat_feature not in selected_orig_features:\n",
        "                        selected_orig_features.append(cat_feature)\n",
        "\n",
        "        # Select corresponding columns in X\n",
        "        selected_cols = [f for f in selected_orig_features if f in X.columns]\n",
        "\n",
        "        return selected_cols, X[selected_cols]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature selection for {name}: {str(e)}\")\n",
        "        return features, X\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, name, features, numeric_features, categorical_features):\n",
        "    try:\n",
        "        if name == 'Random Forest':\n",
        "            importance = model.named_steps['rf'].feature_importances_\n",
        "        else:  # XGBoost\n",
        "            importance = model.named_steps['xgb'].feature_importances_\n",
        "\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = []\n",
        "        poly = preprocessor.named_transformers_['num'].named_steps['poly']\n",
        "        poly_features = poly.get_feature_names_out(numeric_features)\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = list(poly_features) + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "        plt.title(f'Feature Importance in {name} Model (Combined Stations)')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'feature_importance_combined_stations_{name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "        return feature_importance.head(3)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting feature importance for {name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred, name):\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual NO2 Ground Concentration (Scaled)')\n",
        "        plt.ylabel('Predicted NO2 Ground Concentration (Scaled)')\n",
        "        plt.title(f'Actual vs Predicted NO2 Ground Concentrations ({name}, Combined Stations)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'actual_vs_predicted_combined_stations_{name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting actual vs predicted for {name}: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    result = load_and_preprocess_data(file_path, exclude_stations=['TV_Center', 'Tv_Center'])\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to preprocess data. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X, y, df, features, preprocessor, target_scaler = result\n",
        "    print(f\"Combined data from stations: {df['Station'].unique().tolist()}\")\n",
        "\n",
        "    # Train models\n",
        "    print(\"Training models...\")\n",
        "    result = train_models(X, y, preprocessor, df)\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to train models. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X_train, X_test, y_train, y_test, results = result\n",
        "\n",
        "    # Process each model\n",
        "    numeric_features = [f for f in features if f not in ['Station', 'Cluster']]\n",
        "    categorical_features = [f for f in ['Station', 'Cluster'] if f in features]\n",
        "\n",
        "    for name, res in results.items():\n",
        "        print(f\"\\nResults for {name} (Combined Stations):\")\n",
        "        print(f\"Best Parameters: {res['best_params']}\")\n",
        "        print(f\"Mean Squared Error: {res['mse']:.4f}\")\n",
        "        print(f\"R-squared Score: {res['r2']:.4f}\")\n",
        "        print(f\"Cross-validated R-squared Scores: {res['cv_scores']}\")\n",
        "        print(f\"Mean CV R-squared: {res['cv_scores'].mean():.4f}\")\n",
        "\n",
        "        # Feature selection\n",
        "        selected_features, X_selected = select_important_features(\n",
        "            res['model'], name, features, numeric_features, categorical_features, X, threshold=0.05\n",
        "        )\n",
        "        print(f\"Selected Features: {selected_features}\")\n",
        "\n",
        "        # Plot feature importance\n",
        "        top_features = plot_feature_importance(res['model'], name, features, numeric_features, categorical_features)\n",
        "        if top_features is not None:\n",
        "            print(f\"Top 3 Features:\\n{top_features}\")\n",
        "\n",
        "        # Plot actual vs predicted\n",
        "        plot_actual_vs_predicted(y_test, res['y_pred'], name)\n",
        "\n",
        "        # Per-station performance\n",
        "        print(f\"\\nPer-Station Performance for {name}:\")\n",
        "        for station, metrics in res['station_metrics'].items():\n",
        "            print(f\"Station {station}: MSE = {metrics['MSE']:.4f}, R2 = {metrics['R2']:.4f}\")\n",
        "\n",
        "        print(f\"Plots saved as 'feature_importance_combined_stations_{name.lower().replace(' ', '_')}.png' and 'actual_vs_predicted_combined_stations_{name.lower().replace(' ', '_')}.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU7VsftsEFgM",
        "outputId": "83e9a177-0a8b-4778-9268-1910ec8b6cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Correlation between NO2_Satellite and NO2_Ground in combined data (after transformations): 0.0798\n",
            "Combined data from stations: ['agrabad', 'barisal', 'cumilla', 'darussalam', 'doe', 'gazipur', 'khulna', 'mymensingh', 'ng', 'ns', 'rangpur', 'savar', 'sylhet', 'rajshahi', 'brac']\n",
            "Training models...\n",
            "\n",
            "Results for Random Forest (Combined Stations):\n",
            "Best Parameters: {'rf__max_depth': 15, 'rf__max_features': 0.5, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 300}\n",
            "Mean Squared Error: 0.0077\n",
            "R-squared Score: 0.6435\n",
            "Cross-validated R-squared Scores: [0.64190297 0.64809426 0.62493411 0.64056884 0.65093233]\n",
            "Mean CV R-squared: 0.6413\n",
            "Selected Features: ['Solar_Radiation', 'Station']\n",
            "Top 3 Features:\n",
            "              Feature  Importance\n",
            "90    Station_agrabad    0.103006\n",
            "35  Solar_Radiation^2    0.078885\n",
            "97     Station_khulna    0.040329\n",
            "\n",
            "Per-Station Performance for Random Forest:\n",
            "Station doe: MSE = 0.0148, R2 = 0.2085\n",
            "Station rajshahi: MSE = 0.0104, R2 = 0.3329\n",
            "Station darussalam: MSE = 0.0039, R2 = 0.3996\n",
            "Station cumilla: MSE = 0.0048, R2 = 0.4694\n",
            "Station ns: MSE = 0.0070, R2 = 0.4692\n",
            "Station rangpur: MSE = 0.0157, R2 = 0.2874\n",
            "Station mymensingh: MSE = 0.0119, R2 = 0.4018\n",
            "Station savar: MSE = 0.0107, R2 = 0.5200\n",
            "Station khulna: MSE = 0.0002, R2 = 0.7246\n",
            "Station ng: MSE = 0.0032, R2 = 0.4392\n",
            "Station barisal: MSE = 0.0044, R2 = 0.6303\n",
            "Station gazipur: MSE = 0.0003, R2 = 0.6045\n",
            "Station agrabad: MSE = 0.0127, R2 = 0.5582\n",
            "Station sylhet: MSE = 0.0095, R2 = 0.5354\n",
            "Station brac: MSE = 0.0271, R2 = 0.3665\n",
            "Plots saved as 'feature_importance_combined_stations_random_forest.png' and 'actual_vs_predicted_combined_stations_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Combined Stations):\n",
            "Best Parameters: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 300, 'xgb__subsample': 0.8}\n",
            "Mean Squared Error: 0.0074\n",
            "R-squared Score: 0.6576\n",
            "Cross-validated R-squared Scores: [0.65992095 0.68082173 0.64553817 0.68301007 0.6783054 ]\n",
            "Mean CV R-squared: 0.6695\n",
            "Selected Features: ['Station']\n",
            "Top 3 Features:\n",
            "               Feature  Importance\n",
            "90     Station_agrabad    0.179384\n",
            "94  Station_darussalam    0.104785\n",
            "93     Station_cumilla    0.095856\n",
            "\n",
            "Per-Station Performance for XGBoost:\n",
            "Station doe: MSE = 0.0133, R2 = 0.2897\n",
            "Station rajshahi: MSE = 0.0097, R2 = 0.3776\n",
            "Station darussalam: MSE = 0.0039, R2 = 0.3971\n",
            "Station cumilla: MSE = 0.0051, R2 = 0.4369\n",
            "Station ns: MSE = 0.0066, R2 = 0.5018\n",
            "Station rangpur: MSE = 0.0113, R2 = 0.4875\n",
            "Station mymensingh: MSE = 0.0103, R2 = 0.4797\n",
            "Station savar: MSE = 0.0110, R2 = 0.5061\n",
            "Station khulna: MSE = 0.0006, R2 = 0.3666\n",
            "Station ng: MSE = 0.0030, R2 = 0.4672\n",
            "Station barisal: MSE = 0.0046, R2 = 0.6122\n",
            "Station gazipur: MSE = 0.0005, R2 = 0.3990\n",
            "Station agrabad: MSE = 0.0156, R2 = 0.4562\n",
            "Station sylhet: MSE = 0.0098, R2 = 0.5174\n",
            "Station brac: MSE = 0.0197, R2 = 0.5388\n",
            "Plots saved as 'feature_importance_combined_stations_xgboost.png' and 'actual_vs_predicted_combined_stations_xgboost.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the combined data\n",
        "def load_and_preprocess_data(file_path, exclude_stations=['tv_center'], min_samples=10):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Standardize station names to lowercase\n",
        "        df['Station'] = df['Station'].str.lower()\n",
        "        exclude_stations = [s.lower() for s in exclude_stations]\n",
        "\n",
        "        # Exclude specified stations\n",
        "        df = df[~df['Station'].isin(exclude_stations)]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available after excluding stations {exclude_stations}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Check for constant or insufficient target data\n",
        "        if df['NO2_Ground'].nunique() <= 1:\n",
        "            print(f\"Target NO2_Ground is constant in combined data. Skipping modeling.\")\n",
        "            return None, None, None, None, None, None\n",
        "        if len(df) < min_samples:\n",
        "            print(f\"Insufficient samples ({len(df)}) in combined data. Minimum required: {min_samples}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Converting Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Select numeric columns for averaging\n",
        "        numeric_columns = ['NO2_Ground', 'NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                          'Temperature', 'Wind_Speed', 'Wind_Direction']\n",
        "        df_numeric = df[['Date', 'Station'] + [col for col in numeric_columns if col in df.columns]]\n",
        "\n",
        "        # Handling duplicates by averaging numeric columns per Date and Station\n",
        "        df_numeric = df_numeric.groupby(['Date', 'Station']).mean().reset_index()\n",
        "\n",
        "        # Log-transform NO2_Satellite with offset\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            df_numeric['NO2_Satellite'] = np.log1p(df_numeric['NO2_Satellite'].clip(lower=0) + 0.1)\n",
        "\n",
        "        # Handling missing values\n",
        "        for col in numeric_columns:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = df_numeric[col].fillna(df_numeric[col].mean())\n",
        "        if 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Direction'] = df_numeric['Wind_Direction'].fillna(df_numeric['Wind_Direction'].median())\n",
        "\n",
        "        # Log-transform skewed features\n",
        "        for col in ['Relative_Humidity', 'Solar_Radiation', 'Wind_Speed']:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = np.log1p(df_numeric[col].clip(lower=0))  # Avoid log(0)\n",
        "\n",
        "        # Scale NO2_Ground\n",
        "        target_scaler = StandardScaler()\n",
        "        df_numeric['NO2_Ground'] = target_scaler.fit_transform(df_numeric[['NO2_Ground']])\n",
        "\n",
        "        # Station-specific outlier removal for low-performing stations (less aggressive)\n",
        "        def remove_outliers(df, column, stations, iqr_factor=2.0):\n",
        "            for station in stations:\n",
        "                mask = df['Station'] == station\n",
        "                if mask.sum() > 0:\n",
        "                    Q1 = df.loc[mask, column].quantile(0.25)\n",
        "                    Q3 = df.loc[mask, column].quantile(0.75)\n",
        "                    IQR = Q3 - Q1\n",
        "                    lower_bound = Q1 - iqr_factor * IQR\n",
        "                    upper_bound = Q3 + iqr_factor * IQR\n",
        "                    df = df[~mask | ((df[column] >= lower_bound) & (df[column] <= upper_bound))]\n",
        "            return df\n",
        "\n",
        "        df_numeric = remove_outliers(df_numeric, 'NO2_Ground', ['doe', 'rajshahi', 'rangpur'])\n",
        "\n",
        "        # Check for constant features and remove them\n",
        "        for col in df_numeric.columns:\n",
        "            if col not in ['Date', 'Station', 'NO2_Ground'] and df_numeric[col].nunique() <= 1:\n",
        "                print(f\"Feature {col} is constant in combined data. Dropping.\")\n",
        "                df_numeric = df_numeric.drop(columns=[col])\n",
        "\n",
        "        # Feature engineering\n",
        "        df_numeric['Month'] = df_numeric['Date'].dt.month\n",
        "        df_numeric['DayOfWeek'] = df_numeric['Date'].dt.dayofweek\n",
        "        if 'Temperature' in df_numeric.columns and 'Relative_Humidity' in df_numeric.columns:\n",
        "            df_numeric['Temp_RH_Interaction'] = df_numeric['Temperature'] * df_numeric['Relative_Humidity']\n",
        "        else:\n",
        "            df_numeric['Temp_RH_Interaction'] = 0\n",
        "        if 'Wind_Speed' in df_numeric.columns and 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Speed_Direction'] = df_numeric['Wind_Speed'] * np.cos(np.radians(df_numeric['Wind_Direction']))\n",
        "        else:\n",
        "            df_numeric['Wind_Speed_Direction'] = 0\n",
        "        if 'NO2_Satellite' in df_numeric.columns and 'Solar_Radiation' in df_numeric.columns:\n",
        "            df_numeric['Satellite_Solar_Interaction'] = df_numeric['NO2_Satellite'] * df_numeric['Solar_Radiation']\n",
        "        else:\n",
        "            df_numeric['Satellite_Solar_Interaction'] = 0\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            df_numeric['Satellite_Month_Interaction'] = df_numeric['NO2_Satellite'] * df_numeric['Month']\n",
        "        else:\n",
        "            df_numeric['Satellite_Month_Interaction'] = 0\n",
        "\n",
        "        # Add lagged NO2_Satellite\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            df_numeric['NO2_Satellite_Lag1'] = df_numeric.groupby('Station')['NO2_Satellite'].shift(1)\n",
        "            df_numeric['NO2_Satellite_Lag1'] = df_numeric['NO2_Satellite_Lag1'].fillna(df_numeric['NO2_Satellite'].mean())\n",
        "\n",
        "        # Station clustering\n",
        "        cluster_features = ['NO2_Ground', 'NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature']\n",
        "        cluster_data = df_numeric[[f for f in cluster_features if f in df_numeric.columns]].fillna(0)\n",
        "        if len(cluster_data) > 0:\n",
        "            kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "            df_numeric['Cluster'] = kmeans.fit_predict(cluster_data)\n",
        "        else:\n",
        "            df_numeric['Cluster'] = 0\n",
        "\n",
        "        # Add NO2_Satellite × Cluster interaction\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            for cluster in df_numeric['Cluster'].unique():\n",
        "                df_numeric[f'Satellite_Cluster_{cluster}'] = df_numeric['NO2_Satellite'] * (df_numeric['Cluster'] == cluster).astype(int)\n",
        "\n",
        "        # Selecting features and target\n",
        "        features = ['NO2_Satellite', 'NO2_Satellite_Lag1', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                    'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek',\n",
        "                    'Temp_RH_Interaction', 'Wind_Speed_Direction', 'Satellite_Solar_Interaction',\n",
        "                    'Satellite_Month_Interaction', 'Station', 'Cluster'] + \\\n",
        "                   [f for f in df_numeric.columns if f.startswith('Satellite_Cluster_')]\n",
        "        features = [f for f in features if f in df_numeric.columns]\n",
        "        if not features or 'NO2_Ground' not in df_numeric.columns:\n",
        "            print(f\"Insufficient features or target.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Correlation analysis\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            correlation = df_numeric[['NO2_Ground', 'NO2_Satellite']].corr().iloc[0, 1]\n",
        "            print(f\"Correlation between NO2_Satellite and NO2_Ground (after log transform with offset): {correlation:.4f}\")\n",
        "\n",
        "        X = df_numeric[features]\n",
        "        y = df_numeric['NO2_Ground']\n",
        "\n",
        "        # Define sample weights based on station performance\n",
        "        station_weights = {\n",
        "            'khulna': 1.5, 'gazipur': 1.5, 'barisal': 1.3, 'brac': 1.2, 'agrabad': 1.2,\n",
        "            'doe': 0.8, 'rajshahi': 0.8, 'rangpur': 0.8\n",
        "        }\n",
        "        sample_weights = df_numeric['Station'].map(station_weights).fillna(1.0)\n",
        "\n",
        "        # Define preprocessing pipeline\n",
        "        numeric_features = [f for f in features if f not in ['Station', 'Cluster']]\n",
        "        categorical_features = [f for f in ['Station', 'Cluster'] if f in features]\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', StandardScaler(), numeric_features),\n",
        "                ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
        "            ])\n",
        "\n",
        "        return X, y, df_numeric, features, preprocessor, target_scaler, sample_weights\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing data: {str(e)}\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "# Training and comparing models\n",
        "def train_models(X, y, preprocessor, df, sample_weights):\n",
        "    try:\n",
        "        # Splitting the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Get station labels and weights for test set\n",
        "        test_indices = X_test.index\n",
        "        stations_test = df.loc[test_indices, 'Station']\n",
        "        weights_train = sample_weights.loc[X_train.index]\n",
        "\n",
        "        # Define models and pipelines\n",
        "        models = {\n",
        "            'Random Forest': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('rf', RandomForestRegressor(random_state=42))\n",
        "            ]),\n",
        "            'XGBoost': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('xgb', XGBRegressor(random_state=42))\n",
        "            ])\n",
        "        }\n",
        "\n",
        "        # Parameter grids\n",
        "        rf_param_grid = {\n",
        "            'rf__n_estimators': [100, 200, 300],\n",
        "            'rf__max_depth': [5, 10, 12],\n",
        "            'rf__min_samples_split': [2, 5],\n",
        "            'rf__min_samples_leaf': [1, 2],\n",
        "            'rf__max_features': ['sqrt', 0.5]\n",
        "        }\n",
        "        xgb_param_grid = {\n",
        "            'xgb__n_estimators': [100, 200, 300],\n",
        "            'xgb__max_depth': [3, 5, 7],\n",
        "            'xgb__learning_rate': [0.01, 0.05, 0.1],\n",
        "            'xgb__subsample': [0.8, 1.0],\n",
        "            'xgb__reg_lambda': [0.1, 1.0, 10.0]\n",
        "        }\n",
        "\n",
        "        # Grid search for Random Forest and XGBoost\n",
        "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        results = {}\n",
        "        for name, pipeline in models.items():\n",
        "            param_grid = rf_param_grid if name == 'Random Forest' else xgb_param_grid\n",
        "            grid_search = GridSearchCV(pipeline, param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "            grid_search.fit(X_train, y_train, **{'rf__sample_weight': weights_train} if name == 'Random Forest' else {})\n",
        "            best_model = grid_search.best_estimator_\n",
        "            best_params = grid_search.best_params_\n",
        "\n",
        "            # Predictions and metrics\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            cv_scores = cross_val_score(best_model, X, y, cv=kfold, scoring='r2')\n",
        "\n",
        "            # Per-station performance\n",
        "            station_metrics = {}\n",
        "            for station in stations_test.unique():\n",
        "                mask = stations_test == station\n",
        "                if mask.sum() > 0:\n",
        "                    station_mse = mean_squared_error(y_test[mask], y_pred[mask])\n",
        "                    station_r2 = r2_score(y_test[mask], y_pred[mask])\n",
        "                    station_metrics[station] = {'MSE': station_mse, 'R2': station_r2}\n",
        "\n",
        "            results[name] = {\n",
        "                'model': best_model,\n",
        "                'y_pred': y_pred,\n",
        "                'mse': mse,\n",
        "                'r2': r2,\n",
        "                'cv_scores': cv_scores,\n",
        "                'best_params': best_params,\n",
        "                'station_metrics': station_metrics\n",
        "            }\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, results\n",
        "    except Exception as e:\n",
        "        print(f\"Error training models: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Feature selection based on importance\n",
        "def select_important_features(model, name, features, numeric_features, categorical_features, X, threshold=0.03):\n",
        "    try:\n",
        "        if name == 'Random Forest':\n",
        "            importance = model.named_steps['rf'].feature_importances_\n",
        "        else:  # XGBoost\n",
        "            importance = model.named_steps['xgb'].feature_importances_\n",
        "\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = numeric_features\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = num_features + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        selected_features = feature_importance[feature_importance['Importance'] >= threshold]['Feature'].tolist()\n",
        "\n",
        "        if not selected_features:\n",
        "            return features, X\n",
        "\n",
        "        # Ensure NO2_Satellite-related features are included\n",
        "        satellite_features = [f for f in all_features if 'NO2_Satellite' in f]\n",
        "        if not any('NO2_Satellite' in f for f in selected_features):\n",
        "            selected_features.extend(satellite_features[:2])  # Include top 2 NO2_Satellite features\n",
        "\n",
        "        # Map back to original feature names\n",
        "        selected_orig_features = []\n",
        "        for f in selected_features:\n",
        "            if f in num_features:\n",
        "                selected_orig_features.append(f)\n",
        "            else:\n",
        "                for cat_feature in categorical_features:\n",
        "                    if f.startswith(cat_feature) and cat_feature not in selected_orig_features:\n",
        "                        selected_orig_features.append(cat_feature)\n",
        "\n",
        "        # Select corresponding columns in X\n",
        "        selected_cols = [f for f in selected_orig_features if f in X.columns]\n",
        "\n",
        "        return selected_cols, X[selected_cols]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature selection for {name}: {str(e)}\")\n",
        "        return features, X\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, name, features, numeric_features, categorical_features):\n",
        "    try:\n",
        "        if name == 'Random Forest':\n",
        "            importance = model.named_steps['rf'].feature_importances_\n",
        "        else:  # XGBoost\n",
        "            importance = model.named_steps['xgb'].feature_importances_\n",
        "\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = numeric_features\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = num_features + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "        plt.title(f'Feature Importance in {name} Model (Combined Stations)')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'feature_importance_combined_stations_{name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "        return feature_importance.head(3)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting feature importance for {name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred, name):\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual NO2 Ground Concentration (Scaled)')\n",
        "        plt.ylabel('Predicted NO2 Ground Concentration (Scaled)')\n",
        "        plt.title(f'Actual vs Predicted NO2 Ground Concentrations ({name}, Combined Stations)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'actual_vs_predicted_combined_stations_{name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting actual vs predicted for {name}: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    result = load_and_preprocess_data(file_path, exclude_stations=['TV_Center', 'Tv_Center'])\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to preprocess data. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X, y, df, features, preprocessor, target_scaler, sample_weights = result\n",
        "    print(f\"Combined data from stations: {df['Station'].unique().tolist()}\")\n",
        "\n",
        "    # Train models\n",
        "    print(\"Training models...\")\n",
        "    result = train_models(X, y, preprocessor, df, sample_weights)\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to train models. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X_train, X_test, y_train, y_test, results = result\n",
        "\n",
        "    # Process each model\n",
        "    numeric_features = [f for f in features if f not in ['Station', 'Cluster']]\n",
        "    categorical_features = [f for f in ['Station', 'Cluster'] if f in features]\n",
        "\n",
        "    for name, res in results.items():\n",
        "        print(f\"\\nResults for {name} (Combined Stations):\")\n",
        "        print(f\"Best Parameters: {res['best_params']}\")\n",
        "        print(f\"Mean Squared Error: {res['mse']:.4f}\")\n",
        "        print(f\"R-squared Score: {res['r2']:.4f}\")\n",
        "        print(f\"Cross-validated R-squared Scores: {res['cv_scores']}\")\n",
        "        print(f\"Mean CV R-squared: {res['cv_scores'].mean():.4f}\")\n",
        "\n",
        "        # Feature selection\n",
        "        selected_features, X_selected = select_important_features(\n",
        "            res['model'], name, features, numeric_features, categorical_features, X, threshold=0.03\n",
        "        )\n",
        "        print(f\"Selected Features: {selected_features}\")\n",
        "\n",
        "        # Plot feature importance\n",
        "        top_features = plot_feature_importance(res['model'], name, features, numeric_features, categorical_features)\n",
        "        if top_features is not None:\n",
        "            print(f\"Top 3 Features:\\n{top_features}\")\n",
        "\n",
        "        # Plot actual vs predicted\n",
        "        plot_actual_vs_predicted(y_test, res['y_pred'], name)\n",
        "\n",
        "        # Per-station performance\n",
        "        print(f\"\\nPer-Station Performance for {name}:\")\n",
        "        for station, metrics in res['station_metrics'].items():\n",
        "            print(f\"Station {station}: MSE = {metrics['MSE']:.4f}, R2 = {metrics['R2']:.4f}\")\n",
        "\n",
        "        print(f\"Plots saved as 'feature_importance_combined_stations_{name.lower().replace(' ', '_')}.png' and 'actual_vs_predicted_combined_stations_{name.lower().replace(' ', '_')}.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgV0GpAoHF_X",
        "outputId": "05193fd5-3919-4413-98b0-122bcbd41c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Correlation between NO2_Satellite and NO2_Ground (after log transform with offset): 0.1398\n",
            "Combined data from stations: ['agrabad', 'barisal', 'brac', 'cumilla', 'darussalam', 'doe', 'gazipur', 'khulna', 'mymensingh', 'ng', 'ns', 'rangpur', 'savar', 'sylhet', 'rajshahi']\n",
            "Training models...\n",
            "\n",
            "Results for Random Forest (Combined Stations):\n",
            "Best Parameters: {'rf__max_depth': 12, 'rf__max_features': 0.5, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 300}\n",
            "Mean Squared Error: 0.3762\n",
            "R-squared Score: 0.6172\n",
            "Cross-validated R-squared Scores: [0.62266605 0.71588373 0.63722536 0.52991281 0.71701038]\n",
            "Mean CV R-squared: 0.6445\n",
            "Selected Features: ['NO2_Satellite', 'NO2_Satellite_Lag1', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Temp_RH_Interaction', 'Wind_Speed_Direction', 'Satellite_Solar_Interaction', 'Satellite_Month_Interaction', 'Satellite_Cluster_1', 'Station']\n",
            "Top 3 Features:\n",
            "                Feature  Importance\n",
            "19         Station_brac    0.345347\n",
            "9   Temp_RH_Interaction    0.064298\n",
            "2     Relative_Humidity    0.055897\n",
            "\n",
            "Per-Station Performance for Random Forest:\n",
            "Station sylhet: MSE = 0.0139, R2 = 0.3856\n",
            "Station cumilla: MSE = 0.0319, R2 = -2.1133\n",
            "Station savar: MSE = 0.1223, R2 = -0.0302\n",
            "Station gazipur: MSE = 0.0003, R2 = 0.6709\n",
            "Station ns: MSE = 2.1720, R2 = -0.0101\n",
            "Station rajshahi: MSE = 0.0020, R2 = -0.4903\n",
            "Station ng: MSE = 0.0046, R2 = 0.4453\n",
            "Station khulna: MSE = 0.2221, R2 = 0.5294\n",
            "Station barisal: MSE = 0.0166, R2 = 0.3081\n",
            "Station mymensingh: MSE = 1.7479, R2 = 0.1523\n",
            "Station rangpur: MSE = 0.0176, R2 = -0.1461\n",
            "Station darussalam: MSE = 0.0093, R2 = 0.1871\n",
            "Station brac: MSE = 0.9554, R2 = 0.6791\n",
            "Station agrabad: MSE = 0.0259, R2 = 0.6085\n",
            "Station doe: MSE = 0.0216, R2 = 0.1081\n",
            "Plots saved as 'feature_importance_combined_stations_random_forest.png' and 'actual_vs_predicted_combined_stations_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Combined Stations):\n",
            "Best Parameters: {'xgb__learning_rate': 0.05, 'xgb__max_depth': 7, 'xgb__n_estimators': 300, 'xgb__reg_lambda': 1.0, 'xgb__subsample': 0.8}\n",
            "Mean Squared Error: 0.4154\n",
            "R-squared Score: 0.5773\n",
            "Cross-validated R-squared Scores: [0.55419592 0.76049983 0.67346563 0.55814772 0.71616366]\n",
            "Mean CV R-squared: 0.6525\n",
            "Selected Features: ['Month', 'Station', 'NO2_Satellite', 'NO2_Satellite_Lag1']\n",
            "Top 3 Features:\n",
            "                Feature  Importance\n",
            "19         Station_brac    0.682298\n",
            "7                 Month    0.035382\n",
            "9   Temp_RH_Interaction    0.029942\n",
            "\n",
            "Per-Station Performance for XGBoost:\n",
            "Station sylhet: MSE = 0.0124, R2 = 0.4529\n",
            "Station cumilla: MSE = 0.0271, R2 = -1.6440\n",
            "Station savar: MSE = 0.1046, R2 = 0.1190\n",
            "Station gazipur: MSE = 0.0005, R2 = 0.4178\n",
            "Station ns: MSE = 2.5900, R2 = -0.2045\n",
            "Station rajshahi: MSE = 0.0011, R2 = 0.1807\n",
            "Station ng: MSE = 0.0049, R2 = 0.4125\n",
            "Station khulna: MSE = 0.1009, R2 = 0.7861\n",
            "Station barisal: MSE = 0.0171, R2 = 0.2876\n",
            "Station mymensingh: MSE = 2.0336, R2 = 0.0138\n",
            "Station rangpur: MSE = 0.0146, R2 = 0.0504\n",
            "Station darussalam: MSE = 0.0072, R2 = 0.3708\n",
            "Station brac: MSE = 0.9768, R2 = 0.6719\n",
            "Station agrabad: MSE = 0.0267, R2 = 0.5960\n",
            "Station doe: MSE = 0.0198, R2 = 0.1841\n",
            "Plots saved as 'feature_importance_combined_stations_xgboost.png' and 'actual_vs_predicted_combined_stations_xgboost.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading and preprocessing the combined data\n",
        "def load_and_preprocess_data(file_path, exclude_stations=['tv_center', 'doe', 'ns', 'cumilla'], min_samples=10):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Standardize station names to lowercase\n",
        "        df['Station'] = df['Station'].str.lower()\n",
        "        exclude_stations = [s.lower() for s in exclude_stations]\n",
        "\n",
        "        # Exclude specified stations\n",
        "        df = df[~df['Station'].isin(exclude_stations)]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available after excluding stations {exclude_stations}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Check for constant or insufficient target data\n",
        "        if df['NO2_Ground'].nunique() <= 1:\n",
        "            print(f\"Target NO2_Ground is constant in combined data. Skipping modeling.\")\n",
        "            return None, None, None, None, None, None\n",
        "        if len(df) < min_samples:\n",
        "            print(f\"Insufficient samples ({len(df)}) in combined data. Minimum required: {min_samples}.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Converting Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Select numeric columns for averaging\n",
        "        numeric_columns = ['NO2_Ground', 'NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                          'Temperature', 'Wind_Speed', 'Wind_Direction']\n",
        "        df_numeric = df[['Date', 'Station'] + [col for col in numeric_columns if col in df.columns]]\n",
        "\n",
        "        # Handling duplicates by averaging numeric columns per Date and Station\n",
        "        df_numeric = df_numeric.groupby(['Date', 'Station']).mean().reset_index()\n",
        "\n",
        "        # Square root transform NO2_Satellite\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            df_numeric['NO2_Satellite'] = np.sqrt(df_numeric['NO2_Satellite'].clip(lower=0))\n",
        "\n",
        "        # Handling missing values\n",
        "        for col in numeric_columns:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = df_numeric[col].fillna(df_numeric[col].mean())\n",
        "        if 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Direction'] = df_numeric['Wind_Direction'].fillna(df_numeric['Wind_Direction'].median())\n",
        "\n",
        "        # Log-transform skewed features\n",
        "        for col in ['Relative_Humidity', 'Solar_Radiation', 'Wind_Speed']:\n",
        "            if col in df_numeric.columns:\n",
        "                df_numeric[col] = np.log1p(df_numeric[col].clip(lower=0))  # Avoid log(0)\n",
        "\n",
        "        # Scale NO2_Ground\n",
        "        target_scaler = StandardScaler()\n",
        "        df_numeric['NO2_Ground'] = target_scaler.fit_transform(df_numeric[['NO2_Ground']])\n",
        "\n",
        "        # Station-specific outlier removal for low-performing stations\n",
        "        def remove_outliers(df, column, stations, iqr_factor=1.5):\n",
        "            for station in stations:\n",
        "                mask = df['Station'] == station\n",
        "                if mask.sum() > 0:\n",
        "                    Q1 = df.loc[mask, column].quantile(0.25)\n",
        "                    Q3 = df.loc[mask, column].quantile(0.75)\n",
        "                    IQR = Q3 - Q1\n",
        "                    lower_bound = Q1 - iqr_factor * IQR\n",
        "                    upper_bound = Q3 + iqr_factor * IQR\n",
        "                    df = df[~mask | ((df[column] >= lower_bound) & (df[column] <= upper_bound))]\n",
        "            return df\n",
        "\n",
        "        df_numeric = remove_outliers(df_numeric, 'NO2_Ground', ['rajshahi', 'rangpur'])\n",
        "\n",
        "        # Check for constant features and remove them\n",
        "        for col in df_numeric.columns:\n",
        "            if col not in ['Date', 'Station', 'NO2_Ground'] and df_numeric[col].nunique() <= 1:\n",
        "                print(f\"Feature {col} is constant in combined data. Dropping.\")\n",
        "                df_numeric = df_numeric.drop(columns=[col])\n",
        "\n",
        "        # Feature engineering\n",
        "        df_numeric['Month'] = df_numeric['Date'].dt.month\n",
        "        df_numeric['DayOfWeek'] = df_numeric['Date'].dt.dayofweek\n",
        "        if 'Temperature' in df_numeric.columns and 'Relative_Humidity' in df_numeric.columns:\n",
        "            df_numeric['Temp_RH_Interaction'] = df_numeric['Temperature'] * df_numeric['Relative_Humidity']\n",
        "        else:\n",
        "            df_numeric['Temp_RH_Interaction'] = 0\n",
        "        if 'Wind_Speed' in df_numeric.columns and 'Wind_Direction' in df_numeric.columns:\n",
        "            df_numeric['Wind_Speed_Direction'] = df_numeric['Wind_Speed'] * np.cos(np.radians(df_numeric['Wind_Direction']))\n",
        "        else:\n",
        "            df_numeric['Wind_Speed_Direction'] = 0\n",
        "        if 'NO2_Satellite' in df_numeric.columns and 'Solar_Radiation' in df_numeric.columns:\n",
        "            df_numeric['Satellite_Solar_Interaction'] = df_numeric['NO2_Satellite'] * df_numeric['Solar_Radiation']\n",
        "        else:\n",
        "            df_numeric['Satellite_Solar_Interaction'] = 0\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            df_numeric['Satellite_Month_Interaction'] = df_numeric['NO2_Satellite'] * df_numeric['Month']\n",
        "        else:\n",
        "            df_numeric['Satellite_Month_Interaction'] = 0\n",
        "\n",
        "        # Add lagged NO2_Satellite\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            df_numeric['NO2_Satellite_Lag1'] = df_numeric.groupby('Station')['NO2_Satellite'].shift(1)\n",
        "            df_numeric['NO2_Satellite_Lag1'] = df_numeric['NO2_Satellite_Lag1'].fillna(df_numeric['NO2_Satellite'].mean())\n",
        "\n",
        "        # Add NO2_Satellite × Station interactions\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            for station in df_numeric['Station'].unique():\n",
        "                df_numeric[f'Satellite_Station_{station}'] = df_numeric['NO2_Satellite'] * (df_numeric['Station'] == station).astype(int)\n",
        "\n",
        "        # Station clustering\n",
        "        cluster_features = ['NO2_Ground', 'NO2_Satellite', 'Relative_Humidity', 'Solar_Radiation', 'Temperature']\n",
        "        cluster_data = df_numeric[[f for f in cluster_features if f in df_numeric.columns]].fillna(0)\n",
        "        if len(cluster_data) > 0:\n",
        "            kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "            df_numeric['Cluster'] = kmeans.fit_predict(cluster_data)\n",
        "        else:\n",
        "            df_numeric['Cluster'] = 0\n",
        "\n",
        "        # Add NO2_Satellite × Cluster interaction\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            for cluster in df_numeric['Cluster'].unique():\n",
        "                df_numeric[f'Satellite_Cluster_{cluster}'] = df_numeric['NO2_Satellite'] * (df_numeric['Cluster'] == cluster).astype(int)\n",
        "\n",
        "        # Selecting features and target\n",
        "        features = ['NO2_Satellite', 'NO2_Satellite_Lag1', 'Relative_Humidity', 'Solar_Radiation',\n",
        "                    'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'DayOfWeek',\n",
        "                    'Temp_RH_Interaction', 'Wind_Speed_Direction', 'Satellite_Solar_Interaction',\n",
        "                    'Satellite_Month_Interaction', 'Station', 'Cluster'] + \\\n",
        "                   [f for f in df_numeric.columns if f.startswith('Satellite_Station_') or f.startswith('Satellite_Cluster_')]\n",
        "        features = [f for f in features if f in df_numeric.columns]\n",
        "        if not features or 'NO2_Ground' not in df_numeric.columns:\n",
        "            print(f\"Insufficient features or target.\")\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        # Correlation analysis\n",
        "        if 'NO2_Satellite' in df_numeric.columns:\n",
        "            correlation = df_numeric[['NO2_Ground', 'NO2_Satellite']].corr().iloc[0, 1]\n",
        "            print(f\"Correlation between NO2_Satellite and NO2_Ground (after sqrt transform): {correlation:.4f}\")\n",
        "\n",
        "        X = df_numeric[features]\n",
        "        y = df_numeric['NO2_Ground']\n",
        "\n",
        "        # Define preprocessing pipeline\n",
        "        numeric_features = [f for f in features if f not in ['Station', 'Cluster']]\n",
        "        categorical_features = [f for f in ['Station', 'Cluster'] if f in features]\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', StandardScaler(), numeric_features),\n",
        "                ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
        "            ])\n",
        "\n",
        "        return X, y, df_numeric, features, preprocessor, target_scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing data: {str(e)}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "# Training and comparing models\n",
        "def train_models(X, y, preprocessor, df):\n",
        "    try:\n",
        "        # Splitting the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Get station labels for test set\n",
        "        test_indices = X_test.index\n",
        "        stations_test = df.loc[test_indices, 'Station']\n",
        "\n",
        "        # Inverse transform y for evaluation\n",
        "        target_scaler = StandardScaler()\n",
        "        target_scaler.fit(y.values.reshape(-1, 1))\n",
        "        y_train_unscaled = target_scaler.inverse_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "        y_test_unscaled = target_scaler.inverse_transform(y_test.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "        # Define models and pipelines\n",
        "        models = {\n",
        "            'Random Forest': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('rf', RandomForestRegressor(random_state=42))\n",
        "            ]),\n",
        "            'XGBoost': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('xgb', XGBRegressor(random_state=42))\n",
        "            ])\n",
        "        }\n",
        "\n",
        "        # Parameter grids\n",
        "        rf_param_grid = {\n",
        "            'rf__n_estimators': [100, 200, 300],\n",
        "            'rf__max_depth': [5, 10, 12],\n",
        "            'rf__min_samples_split': [2, 5],\n",
        "            'rf__min_samples_leaf': [1, 2],\n",
        "            'rf__max_features': ['sqrt', 0.5]\n",
        "        }\n",
        "        xgb_param_grid = {\n",
        "            'xgb__n_estimators': [100, 200, 300],\n",
        "            'xgb__max_depth': [3, 5],\n",
        "            'xgb__learning_rate': [0.01, 0.05, 0.1],\n",
        "            'xgb__subsample': [0.8, 1.0],\n",
        "            'xgb__reg_lambda': [1.0, 10.0, 50.0]\n",
        "        }\n",
        "\n",
        "        # Grid search for Random Forest and XGBoost\n",
        "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        results = {}\n",
        "        for name, pipeline in models.items():\n",
        "            param_grid = rf_param_grid if name == 'Random Forest' else xgb_param_grid\n",
        "            grid_search = GridSearchCV(pipeline, param_grid, cv=kfold, scoring='r2', n_jobs=-1)\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_model = grid_search.best_estimator_\n",
        "            best_params = grid_search.best_params_\n",
        "\n",
        "            # Predictions\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            y_pred_unscaled = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
        "\n",
        "            # Metrics on unscaled data\n",
        "            mse = mean_squared_error(y_test_unscaled, y_pred_unscaled)\n",
        "            r2 = r2_score(y_test_unscaled, y_pred_unscaled)\n",
        "            cv_scores = cross_val_score(best_model, X, y, cv=kfold, scoring='r2')\n",
        "\n",
        "            # Per-station performance\n",
        "            station_metrics = {}\n",
        "            for station in stations_test.unique():\n",
        "                mask = stations_test == station\n",
        "                if mask.sum() > 0:\n",
        "                    station_mse = mean_squared_error(y_test_unscaled[mask], y_pred_unscaled[mask])\n",
        "                    station_r2 = r2_score(y_test_unscaled[mask], y_pred_unscaled[mask])\n",
        "                    station_metrics[station] = {'MSE': station_mse, 'R2': station_r2}\n",
        "\n",
        "            results[name] = {\n",
        "                'model': best_model,\n",
        "                'y_pred': y_pred_unscaled,\n",
        "                'mse': mse,\n",
        "                'r2': r2,\n",
        "                'cv_scores': cv_scores,\n",
        "                'best_params': best_params,\n",
        "                'station_metrics': station_metrics\n",
        "            }\n",
        "\n",
        "        return X_train, X_test, y_train, y_test_unscaled, results\n",
        "    except Exception as e:\n",
        "        print(f\"Error training models: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Feature selection based on importance\n",
        "def select_important_features(model, name, features, numeric_features, categorical_features, X, threshold=0.02):\n",
        "    try:\n",
        "        if name == 'Random Forest':\n",
        "            importance = model.named_steps['rf'].feature_importances_\n",
        "        else:  # XGBoost\n",
        "            importance = model.named_steps['xgb'].feature_importances_\n",
        "\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = numeric_features\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = num_features + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        selected_features = feature_importance[feature_importance['Importance'] >= threshold]['Feature'].tolist()\n",
        "\n",
        "        if not selected_features:\n",
        "            return features, X\n",
        "\n",
        "        # Ensure NO2_Satellite-related features are included\n",
        "        satellite_features = [f for f in all_features if 'NO2_Satellite' in f]\n",
        "        if not any('NO2_Satellite' in f for f in selected_features):\n",
        "            selected_features.extend(satellite_features[:3])  # Include top 3 NO2_Satellite features\n",
        "\n",
        "        # Map back to original feature names\n",
        "        selected_orig_features = []\n",
        "        for f in selected_features:\n",
        "            if f in num_features:\n",
        "                selected_orig_features.append(f)\n",
        "            else:\n",
        "                for cat_feature in categorical_features:\n",
        "                    if f.startswith(cat_feature) and cat_feature not in selected_orig_features:\n",
        "                        selected_orig_features.append(cat_feature)\n",
        "\n",
        "        # Select corresponding columns in X\n",
        "        selected_cols = [f for f in selected_orig_features if f in X.columns]\n",
        "\n",
        "        return selected_cols, X[selected_cols]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature selection for {name}: {str(e)}\")\n",
        "        return features, X\n",
        "\n",
        "# Visualizing feature importance\n",
        "def plot_feature_importance(model, name, features, numeric_features, categorical_features):\n",
        "    try:\n",
        "        if name == 'Random Forest':\n",
        "            importance = model.named_steps['rf'].feature_importances_\n",
        "        else:  # XGBoost\n",
        "            importance = model.named_steps['xgb'].feature_importances_\n",
        "\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        num_features = numeric_features\n",
        "        if categorical_features:\n",
        "            cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        else:\n",
        "            cat_feature_names = []\n",
        "        all_features = num_features + cat_feature_names\n",
        "\n",
        "        feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importance})\n",
        "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "        plt.title(f'Feature Importance in {name} Model (Combined Stations)')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'feature_importance_combined_stations_{name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "        return feature_importance.head(3)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting feature importance for {name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Visualizing actual vs predicted values\n",
        "def plot_actual_vs_predicted(y_test, y_pred, name):\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual NO2 Ground Concentration')\n",
        "        plt.ylabel('Predicted NO2 Ground Concentration')\n",
        "        plt.title(f'Actual vs Predicted NO2 Ground Concentrations ({name}, Combined Stations)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'actual_vs_predicted_combined_stations_{name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting actual vs predicted for {name}: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    file_path = 'prepared_air_quality_data.csv'\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    result = load_and_preprocess_data(file_path)\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to preprocess data. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X, y, df, features, preprocessor, target_scaler = result\n",
        "    print(f\"Combined data from stations: {df['Station'].unique().tolist()}\")\n",
        "\n",
        "    # Train models\n",
        "    print(\"Training models...\")\n",
        "    result = train_models(X, y, preprocessor, df)\n",
        "    if result[0] is None:\n",
        "        print(\"Failed to train models. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X_train, X_test, y_train, y_test, results = result\n",
        "\n",
        "    # Process each model\n",
        "    numeric_features = [f for f in features if f not in ['Station', 'Cluster']]\n",
        "    categorical_features = [f for f in ['Station', 'Cluster'] if f in features]\n",
        "\n",
        "    for name, res in results.items():\n",
        "        print(f\"\\nResults for {name} (Combined Stations):\")\n",
        "        print(f\"Best Parameters: {res['best_params']}\")\n",
        "        print(f\"Mean Squared Error: {res['mse']:.4f}\")\n",
        "        print(f\"R-squared Score: {res['r2']:.4f}\")\n",
        "        print(f\"Cross-validated R-squared Scores: {res['cv_scores']}\")\n",
        "        print(f\"Mean CV R-squared: {res['cv_scores'].mean():.4f}\")\n",
        "\n",
        "        # Feature selection\n",
        "        selected_features, X_selected = select_important_features(\n",
        "            res['model'], name, features, numeric_features, categorical_features, X, threshold=0.02\n",
        "        )\n",
        "        print(f\"Selected Features: {selected_features}\")\n",
        "\n",
        "        # Plot feature importance\n",
        "        top_features = plot_feature_importance(res['model'], name, features, numeric_features, categorical_features)\n",
        "        if top_features is not None:\n",
        "            print(f\"Top 3 Features:\\n{top_features}\")\n",
        "\n",
        "        # Plot actual vs predicted\n",
        "        plot_actual_vs_predicted(y_test, res['y_pred'], name)\n",
        "\n",
        "        # Per-station performance\n",
        "        print(f\"\\nPer-Station Performance for {name}:\")\n",
        "        for station, metrics in res['station_metrics'].items():\n",
        "            print(f\"Station {station}: MSE = {metrics['MSE']:.4f}, R2 = {metrics['R2']:.4f}\")\n",
        "\n",
        "        print(f\"Plots saved as 'feature_importance_combined_stations_{name.lower().replace(' ', '_')}.png' and 'actual_vs_predicted_combined_stations_{name.lower().replace(' ', '_')}.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPcnbR5nW98E",
        "outputId": "7ed6cd48-725d-4761-f1eb-ec2af499b35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Correlation between NO2_Satellite and NO2_Ground (after sqrt transform): 0.2235\n",
            "Combined data from stations: ['agrabad', 'barisal', 'brac', 'darussalam', 'gazipur', 'khulna', 'mymensingh', 'ng', 'rangpur', 'savar', 'sylhet', 'rajshahi']\n",
            "Training models...\n",
            "\n",
            "Results for Random Forest (Combined Stations):\n",
            "Best Parameters: {'rf__max_depth': 12, 'rf__max_features': 0.5, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 200}\n",
            "Mean Squared Error: 0.2024\n",
            "R-squared Score: 0.7906\n",
            "Cross-validated R-squared Scores: [0.79452572 0.52283907 0.80130835 0.75305326 0.72426896]\n",
            "Mean CV R-squared: 0.7192\n",
            "Selected Features: ['NO2_Satellite', 'NO2_Satellite_Lag1', 'Relative_Humidity', 'Solar_Radiation', 'Temperature', 'Wind_Speed', 'Wind_Direction', 'Month', 'Temp_RH_Interaction', 'Wind_Speed_Direction', 'Satellite_Solar_Interaction', 'Satellite_Month_Interaction', 'Satellite_Station_brac', 'Satellite_Station_mymensingh', 'Satellite_Cluster_2', 'Station']\n",
            "Top 3 Features:\n",
            "                   Feature  Importance\n",
            "15  Satellite_Station_brac    0.281644\n",
            "30            Station_brac    0.145172\n",
            "9      Temp_RH_Interaction    0.062948\n",
            "\n",
            "Per-Station Performance for Random Forest:\n",
            "Station rangpur: MSE = 0.0135, R2 = 0.2709\n",
            "Station savar: MSE = 0.0258, R2 = -0.1003\n",
            "Station sylhet: MSE = 0.0135, R2 = 0.4365\n",
            "Station darussalam: MSE = 0.0041, R2 = 0.3600\n",
            "Station ng: MSE = 0.0065, R2 = 0.3642\n",
            "Station brac: MSE = 1.3340, R2 = 0.6134\n",
            "Station rajshahi: MSE = 0.0012, R2 = 0.1652\n",
            "Station mymensingh: MSE = 0.6797, R2 = -0.1767\n",
            "Station gazipur: MSE = 0.0011, R2 = 0.5546\n",
            "Station khulna: MSE = 0.2214, R2 = 0.6068\n",
            "Station agrabad: MSE = 0.0222, R2 = 0.6419\n",
            "Station barisal: MSE = 0.0122, R2 = 0.4318\n",
            "Plots saved as 'feature_importance_combined_stations_random_forest.png' and 'actual_vs_predicted_combined_stations_random_forest.png'\n",
            "\n",
            "Results for XGBoost (Combined Stations):\n",
            "Best Parameters: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 300, 'xgb__reg_lambda': 1.0, 'xgb__subsample': 1.0}\n",
            "Mean Squared Error: 0.2443\n",
            "R-squared Score: 0.7473\n",
            "Cross-validated R-squared Scores: [0.74731364 0.60157877 0.79418169 0.75189774 0.79149768]\n",
            "Mean CV R-squared: 0.7373\n",
            "Selected Features: ['Relative_Humidity', 'Solar_Radiation', 'Month', 'Temp_RH_Interaction', 'Satellite_Month_Interaction', 'Satellite_Station_agrabad', 'Satellite_Station_brac', 'Satellite_Station_mymensingh', 'Satellite_Cluster_2', 'Station', 'NO2_Satellite', 'NO2_Satellite_Lag1']\n",
            "Top 3 Features:\n",
            "                   Feature  Importance\n",
            "15  Satellite_Station_brac    0.350155\n",
            "25     Satellite_Cluster_2    0.135073\n",
            "7                    Month    0.069790\n",
            "\n",
            "Per-Station Performance for XGBoost:\n",
            "Station rangpur: MSE = 0.0209, R2 = -0.1278\n",
            "Station savar: MSE = 0.0507, R2 = -1.1623\n",
            "Station sylhet: MSE = 0.0144, R2 = 0.3998\n",
            "Station darussalam: MSE = 0.0055, R2 = 0.1462\n",
            "Station ng: MSE = 0.0077, R2 = 0.2457\n",
            "Station brac: MSE = 1.3816, R2 = 0.5996\n",
            "Station rajshahi: MSE = 0.0051, R2 = -2.6796\n",
            "Station mymensingh: MSE = 1.1137, R2 = -0.9278\n",
            "Station gazipur: MSE = 0.0016, R2 = 0.3437\n",
            "Station khulna: MSE = 0.2158, R2 = 0.6169\n",
            "Station agrabad: MSE = 0.0308, R2 = 0.5031\n",
            "Station barisal: MSE = 0.0064, R2 = 0.7009\n",
            "Plots saved as 'feature_importance_combined_stations_xgboost.png' and 'actual_vs_predicted_combined_stations_xgboost.png'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo9dNF1pj168RWStE2bJcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}